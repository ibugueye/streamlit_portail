import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import random
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import networkx as nx

import requests
import pandas as pd
from datetime import datetime, timedelta

# Configuration de la page
st.set_page_config(
    page_title="Big Data MBA Complet",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# DONN√âES ET FONCTIONS DE BASE
# =============================================================================

def generer_donnees_big_data():
    """G√©n√®re un dataset complet pour les d√©monstrations"""
    np.random.seed(42)
    
    # Donn√©es pour les 5V
    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')
    sectors = ['Retail', 'Finance', 'Sant√©', 'Telecom', 'Transport', '√ânergie']
    regions = ['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor']
    
    data = []
    for date in dates[:100]:  # Volume r√©duit pour la d√©mo
        for sector in sectors:
            for region in regions:
                data.append({
                    'date': date,
                    'secteur': sector,
                    'region': region,
                    'volume_data': np.random.randint(1000, 100000),
                    'vitesse_traitement': np.random.randint(1, 1000),
                    'transactions': np.random.randint(100, 10000),
                    'erreurs': np.random.randint(0, 50),
                    'revenus': np.random.randint(5000, 500000),
                    'clients': np.random.randint(10, 1000)
                })
    
    return pd.DataFrame(data)

def calculer_metrics_5v(df):
    """Calcule les m√©triques des 5V"""
    metrics = {
        'Volume': df['volume_data'].sum(),
        'V√©locit√©': df['vitesse_traitement'].mean(),
        'Vari√©t√©': len(df['secteur'].unique()),
        'V√©racit√©': (1 - (df['erreurs'].sum() / df['transactions'].sum())) * 100,
        'Valeur': df['revenus'].sum() / df['volume_data'].sum() * 1000
    }
    return metrics

def generer_donnees_retail():
    """G√©n√®re des donn√©es r√©alistes pour le retail"""
    np.random.seed(42)
    
    # Donn√©es clients
    n_clients = 1000
    data = {
        'client_id': range(1, n_clients + 1),
        'age': np.random.randint(18, 70, n_clients),
        'revenu_annuel': np.random.normal(50000, 20000, n_clients),
        'anciennete_mois': np.random.randint(1, 60, n_clients),
        'frequence_achats': np.random.poisson(5, n_clients),
        'panier_moyen': np.random.normal(85, 30, n_clients),
        'satisfaction': np.random.randint(1, 10, n_clients),
        'region': np.random.choice(['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor'], n_clients),
        'segment': np.random.choice(['Occasionnel', 'R√©gulier', 'Fid√®le', 'VIP'], n_clients, p=[0.4, 0.3, 0.2, 0.1])
    }
    
    df_clients = pd.DataFrame(data)
    df_clients['revenu_annuel'] = df_clients['revenu_annuel'].clip(20000, 150000)
    df_clients['panier_moyen'] = df_clients['panier_moyen'].clip(20, 200)
    
    # Donn√©es transactions
    transactions = []
    for client_id in range(1, n_clients + 1):
        n_transactions = np.random.poisson(df_clients.loc[client_id-1, 'frequence_achats'])
        for _ in range(n_transactions):
            transactions.append({
                'client_id': client_id,
                'date': datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 365)),
                'montant': np.random.normal(df_clients.loc[client_id-1, 'panier_moyen'], 20),
                'categorie': np.random.choice(['√âlectronique', 'V√™tements', 'Alimentation', 'Maison', 'Loisirs']),
                'canal': np.random.choice(['Online', 'Magasin', 'Mobile'], p=[0.6, 0.3, 0.1])
            })
    
    df_transactions = pd.DataFrame(transactions)
    df_transactions['montant'] = df_transactions['montant'].clip(10, 500)
    
    return df_clients, df_transactions

def generer_donnees_qualite():
    """G√©n√®re des donn√©es avec probl√®mes de qualit√©"""
    np.random.seed(123)
    
    data = {
        'id': range(1, 101),
        'nom': [f'Client_{i}' for i in range(1, 101)],
        'email': [f'client{i}@example.com' if np.random.random() > 0.1 else '' for i in range(1, 101)],
        'telephone': [f'77{np.random.randint(100, 999)}{np.random.randint(1000, 9999)}' if np.random.random() > 0.15 else '' for _ in range(100)],
        'age': [np.random.randint(18, 70) if np.random.random() > 0.05 else np.random.randint(200, 300) for _ in range(100)],
        'salaire': [np.random.normal(50000, 20000) if np.random.random() > 0.08 else -1 for _ in range(100)],
        'ville': [np.random.choice(['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', '']) if np.random.random() > 0.07 else '' for _ in range(100)],
        'date_inscription': [datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(100)]
    }
    
    return pd.DataFrame(data)

# =============================================================================
# SECTION 1: INTRODUCTION AU BIG DATA
# =============================================================================

def section_introduction():
    st.header("üåç Introduction au Big Data")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        ### D√©finition Compl√®te du Big Data
        
        Le **Big Data** (M√©gadonn√©es en fran√ßais) d√©signe des **volumes massifs de donn√©es** 
        si grands et complexes qu'ils ne peuvent pas √™tre trait√©s avec les outils traditionnels.
        
        Le **Big Data** repr√©sente l'ensemble des donn√©es dont le **volume**, la **v√©locit√©** et la **vari√©t√©** 
        n√©cessitent des technologies et m√©thodes analytiques sp√©cifiques pour en extraire de la valeur.

        ### üìñ Analogie Simple
        **Imaginez :**
        - üè¢ **Donn√©es traditionnelles** = Une biblioth√®que
        - üåä **Big Data** = Tous les oc√©ans du monde
        
        On passe d'une gestion manuelle √† une gestion industrielle des donn√©es !

        #### √âvolution du Concept
        - **Ann√©es 2000** : Emergence des 3V (Volume, V√©locit√©, Vari√©t√©)
        - **Ann√©es 2010** : Ajout de la V√©racit√© et de la Valeur
        - **Aujourd'hui** : Int√©gration de l'IA et du Cloud Computing
        """)
    
    with col2:
        st.image("https://via.placeholder.com/300x200/4C78A8/FFFFFF?text=Big+Data+Ecosystem", 
                caption="√âcosyst√®me Big Data Moderne")
    
    # Ajouter un s√©parateur avant les 5V
    st.markdown("---")
    
    # Maintenant afficher les 5V du Big Data
    st.subheader("üìä Les 5V du Big Data : D√©finitions D√©taill√©es")

    # D√©finitions des 5V
    v_definitions = {
        "üì¶ Volume": {
            "definition": "Quantit√© massive de donn√©es g√©n√©r√©es, souvent en t√©raoctets ou p√©taoctets",
            "exemple": "**Walmart** traite 2.5+ p√©taoctets/heure de donn√©es de caisse",
            "defi": "Stockage et traitement distribu√©",
            "details": """
            - **√ânormes quantit√©s** de donn√©es g√©n√©r√©es
            - **Exemples :** 
              - 500 millions de tweets par jour
              - 4,2 milliards de vid√©os YouTube vues quotidiennement
              - 3,5 milliards de recherches Google par jour
            - **Chiffre cl√© :** 2,5 trillions d'octets cr√©√©s chaque jour
            """
        },
        "‚ö° V√©locit√©": {
            "definition": "Vitesse de g√©n√©ration, collecte et traitement des donn√©es",
            "exemple": "**Visa** analyse des milliards de transactions en temps r√©el",
            "defi": "Traitement en temps r√©el",
            "details": """
            - **Vitesse** √† laquelle les donn√©es sont g√©n√©r√©es et trait√©es
            - **Temps r√©el** ou quasi r√©el
            - **Exemple :** 
              - Transactions boursi√®res (microsecondes)
              - Capteurs IoT (flux continu)
              - R√©seaux sociaux (donn√©es streaming)
            """
        },
        "üé≠ Vari√©t√©": {
            "definition": "Diversit√© des formats : structur√©, semi-structur√©, non structur√©",
            "exemple": "**Netflix** combine donn√©es d'abonnement et de visionnage",
            "defi": "Int√©gration multi-sources",
            "details": """
            - **Divers formats** de donn√©es :
              - **Structur√©es** : Bases de donn√©es SQL
              - **Semi-structur√©es** : JSON, XML
              - **Non structur√©es** : Textes, images, vid√©os, audio
            - **Exemple :** Donn√©es GPS, emails, photos, vid√©os surveillance
            """
        },
        "‚úÖ V√©racit√©": {
            "definition": "Qualit√©, fiabilit√© et exactitude des donn√©es",
            "exemple": "Nettoyage des logs de clics pour analyse comportementale",
            "defi": "Gouvernance et qualit√©",
            "details": """
            - **Fiabilit√© et qualit√©** des donn√©es
            - Nettoyage et validation n√©cessaire
            - **Exemple :** 
              - Donn√©es erron√©es ou incompl√®tes
              - Fake news sur les r√©seaux
              - Donn√©es bruit√©es des capteurs
            """
        },
        "üí∞ Valeur": {
            "definition": "B√©n√©fice m√©tier concret extrait des donn√©es",
            "exemple": "Campagne marketing cibl√©e boostant les profits de 25%",
            "defi": "ROI et alignement m√©tier",
            "details": """
            - **Utilit√© business** extraite des donn√©es
            - Insights et d√©cisions strat√©giques
            - **Objectif :** Transformer les donn√©es en valeur
            - **Exemple :** Recommandations Amazon, pr√©dictions m√©t√©o
            """
        }
    }

    # Affichage avec boucle
    for i, (v, details) in enumerate(v_definitions.items(), 1):
        with st.expander(f"### {i}. {v}", expanded=(i == 1)):
            st.markdown(f"**üìñ D√©finition :** {details['definition']}")
            st.markdown(f"**üè¢ Cas pratique :** {details['exemple']}")
            st.markdown(f"**‚ö° D√©fi :** {details['defi']}")
            st.markdown("---")
            st.markdown("**üìã D√©tails compl√©mentaires :**")
            st.markdown(details['details'])


# ====================================================
# SECTION 2: TECHNOLOGIES ET ARCHITECTURES - CORRIG√âE ET COMPL√âT√âE
# =============================================================================

def section_technologies():
    st.header("üõ†Ô∏è Technologies et Architectures Big Data")
    
    tab1, tab2, tab3, tab4 = st.tabs([
        "üèóÔ∏è Architecture", "üíæ Stockage", "‚öôÔ∏è Processing", "üîó Int√©gration"
    ])
    
    with tab1:
        st.subheader("Architectures de R√©f√©rence")
        
        architectures = {
            "Lambda Architecture": {
                "description": "Combine traitement batch et stream processing pour √©quilibrer latence et pr√©cision",
                "composants": ["Batch Layer (Hadoop)", "Speed Layer (Storm/Flink)", "Serving Layer (Cassandra)"],
                "use_case": "Syst√®mes n√©cessitant faible latence et pr√©cision historique",
                "avantages": ["Tol√©rance aux pannes", "Scalabilit√© horizontale", "Donn√©es immuables"],
                "inconvenients": ["Complexit√© de maintenance", "Double logique de traitement"]
            },
            "Kappa Architecture": {
                "description": "Simplification de Lambda avec uniquement du stream processing",
                "composants": ["Stream Processing uniquement (Kafka Streams)"],
                "use_case": "Applications temps r√©el modernes avec replay des donn√©es",
                "avantages": ["Simplicit√© architecturale", "Traitement temps r√©el unique", "Reprocessing facile"],
                "inconvenients": ["D√©pendance forte √† Kafka", "Complexit√© des requ√™tes historiques"]
            },
            "Data Mesh": {
                "description": "Approche d√©centralis√©e et orient√©e domaine pour les donn√©es √† grande √©chelle",
                "composants": ["Domain-oriented Data", "Self-serve Platform", "Federated Governance"],
                "use_case": "Grandes organisations complexes avec multiples domaines m√©tier",
                "avantages": ["Scalabilit√© organisationnelle", "Autonomie des √©quipes", "Meilleure qualit√© des donn√©es"],
                "inconvenients": ["Changement culturel important", "Complexit√© de gouvernance"]
            }
        }
        
        selected_arch = st.selectbox("Choisir une architecture:", list(architectures.keys()))
        
        if selected_arch:
            arch = architectures[selected_arch]
            st.write(f"**Description :** {arch['description']}")
            st.write(f"**Composants :** {', '.join(arch['composants'])}")
            st.write(f"**Cas d'usage :** {arch['use_case']}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("**‚úÖ Avantages:**")
                for avantage in arch['avantages']:
                    st.write(f"- {avantage}")
            with col2:
                st.write("**‚ùå Inconv√©nients:**")
                for inconvenient in arch['inconvenients']:
                    st.write(f"- {inconvenient}")
            
            # Diagramme d'architecture simplifi√©
            if selected_arch == "Lambda Architecture":
                st.subheader("üìä Flux de Donn√©es Lambda")
                st.image("https://via.placeholder.com/600x300/4C78A8/FFFFFF?text=Lambda+Architecture", 
                        caption="Batch Layer + Speed Layer + Serving Layer")
            
            elif selected_arch == "Kappa Architecture":
                st.subheader("üìä Flux de Donn√©es Kappa")
                st.image("https://via.placeholder.com/600x200/FF6B6B/FFFFFF?text=Kappa+Architecture", 
                        caption="Single Stream Processing Pipeline")
    
    with tab2:
        st.subheader("Solutions de Stockage")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            #### Data Lake vs Data Warehouse
            
            | Caract√©ristique | Data Lake | Data Warehouse |
            |----------------|-----------|----------------|
            | **Donn√©es** | Brutes, tous formats | Transform√©es, structur√©es |
            | **Schema** | On-read (flexible) | On-write (rigide) |
            | **Utilisateurs** | Data Scientists | Analystes m√©tier |
            | **Co√ªt** | Faible (object storage) | √âlev√© (compute+storage) |
            | **Performance** | Exploration | Reporting/BI |
            | **Governance** | Complexe | Structur√©e |
            """)
            
            st.markdown("""
            #### Cas d'Usage Recommand√©s
            
            **Data Lake:**
            - Exploration de donn√©es brutes
            - Machine Learning
            - Donn√©es IoT/sensors
            - Logs et √©v√©nements
            
            **Data Warehouse:**
            - Reporting op√©rationnel
            - Tableaux de bord BI
            - Analytics m√©tier
            - Conformit√© r√©glementaire
            """)
        
        with col2:
            st.markdown("""
            #### Technologies Populaires
            
            **Data Lakes:**
            - üü† **Amazon S3** - Stockage objet scalable
            - üîµ **Azure Data Lake** - Int√©gration Microsoft
            - üü¢ **Hadoop HDFS** - On-premise
            - üü£ **Google Cloud Storage** - GCP ecosystem
            
            **Data Warehouses:**
            - ‚ùÑÔ∏è **Snowflake** - Architecture cloud-native
            - üîç **BigQuery** - Serverless, forte int√©gration GCP
            - üî¥ **Redshift** - Optimis√© pour AWS
            - üü° **Azure Synapse** - Suite analytics Microsoft
            
            **Hybrides (Lakehouse):**
            - ‚ö° **Databricks Delta Lake** - ACID sur data lake
            - üî∑ **Azure Synapse** - Lake + Warehouse int√©gr√©s
            - üü§ **Apache Hudi** - Tables transactionnelles
            """)
            
            # Comparaison de co√ªts
            st.subheader("üí∞ Comparaison de Co√ªts (Estimation)")
            cout_data = {
                'Solution': ['S3 + Athena', 'BigQuery', 'Snowflake', 'Redshift'],
                'Stockage (‚Ç¨/To/mois)': [20, 25, 30, 35],
                'Compute (‚Ç¨/heure)': [5, 8, 12, 10],
                'Performance': [3, 5, 4, 4]
            }
            df_cout = pd.DataFrame(cout_data)
            st.dataframe(df_cout.style.background_gradient(subset=['Stockage (‚Ç¨/To/mois)', 'Compute (‚Ç¨/heure)']))
    
    with tab3:
        st.subheader("Frameworks de Processing")
        
        processing_frameworks = {
            "Apache Spark": {
                "type": "Traitement distribu√© en m√©moire",
                "use_cases": ["ETL √† grande √©chelle", "Machine Learning", "Analytics interactif", "Streaming"],
                "langages": ["Python", "Scala", "Java", "R", "SQL"],
                "performance": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "maturite": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "exemple": "Netflix - Traitement de 500B √©v√©nements/jour"
            },
            "Apache Flink": {
                "type": "Stream Processing temps r√©el",
                "use_cases": ["CEP (Complex Event Processing)", "Analytics temps r√©el", "D√©tection fraude", "IoT"],
                "langages": ["Java", "Scala", "Python"],
                "performance": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "maturite": "‚≠ê‚≠ê‚≠ê‚≠ê",
                "exemple": "Uber - Optimisation des trajets en temps r√©el"
            },
            "Apache Kafka": {
                "type": "Message Streaming & Event Streaming",
                "use_cases": ["Data Pipelines", "Event Sourcing", "Microservices", "Log aggregation"],
                "langages": ["Java", "Scala", "Python", "Go"],
                "performance": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "maturite": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "exemple": "LinkedIn - 7Tb+/jour, 4M+ messages/sec"
            },
            "Apache Beam": {
                "type": "Processing unifi√© (batch + stream)",
                "use_cases": ["Pipelines portables", "Traitement unifi√©", "Multi-cloud"],
                "langages": ["Java", "Python", "Go"],
                "performance": "‚≠ê‚≠ê‚≠ê",
                "maturite": "‚≠ê‚≠ê‚≠ê",
                "exemple": "Spotify - Recommandations musicales"
            }
        }
        
        selected_framework = st.selectbox("Choisir un framework:", list(processing_frameworks.keys()))
        
        if selected_framework:
            framework = processing_frameworks[selected_framework]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Type :** {framework['type']}")
                st.write(f"**Performance :** {framework['performance']}")
                st.write(f"**Maturit√© :** {framework['maturite']}")
                st.write(f"**Exemple r√©el :** {framework['exemple']}")
                
                st.write("**Cas d'usage principaux:**")
                for use_case in framework['use_cases']:
                    st.write(f"- {use_case}")
            
            with col2:
                st.write("**Langages support√©s:**")
                for langage in framework['langages']:
                    st.write(f"- {langage}")
                
                # M√©triques de performance simul√©es
                if selected_framework == "Apache Spark":
                    st.metric("Throughput", "100+ Go/sec")
                    st.metric("Latence", "1-10 sec")
                    st.metric("Scalabilit√©", "1000+ n≈ìuds")
                elif selected_framework == "Apache Flink":
                    st.metric("Latence", "< 100 ms")
                    st.metric("Checkpointing", "Exactly-once")
                    st.metric("Backpressure", "Gestion automatique")
                elif selected_framework == "Apache Kafka":
                    st.metric("Throughput", "1M+ msg/sec")
                    st.metric("Latence", "< 10 ms")
                    st.metric("Durabilit√©", "Persistent")
            
            # Exemple de code
            st.subheader("üíª Exemple de Code")
            
            if selected_framework == "Apache Spark":
                code = """
# Lecture de donn√©es
df = spark.read.parquet("s3://data-lake/raw/")
 
# Transformation
result = df.filter(df.age > 18) \\
           .groupBy("department") \\
           .agg(avg("salary").alias("avg_salary"))
 
# √âcriture
result.write.parquet("s3://data-lake/processed/")
                """
                st.code(code, language='python')
            
            elif selected_framework == "Apache Flink":
                code = """
// Stream processing temps r√©el
DataStream<Transaction> transactions = env
    .addSource(new TransactionSource())
    .keyBy(Transaction::getAccountId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .process(new FraudDetectionProcessFunction());
                """
                st.code(code, language='java')
    
    with tab4:
        st.subheader("üîó Int√©gration de Donn√©es et Data Pipelines")
        
        st.markdown("""
        ### Architecture d'Int√©gration Moderne
        
        L'int√©gration de donn√©es connecte les sources disparates et assure la circulation 
        des donn√©es dans l'√©cosyst√®me Big Data.
        """)
        
        integration_type = st.selectbox(
            "Type d'int√©gration:",
            ["ETL/ELT", "CDC (Change Data Capture)", "API Integration", "Data Virtualization", "Data Replication"]
        )
        
        if integration_type == "ETL/ELT":
            st.success("**üîÑ ETL (Extract-Transform-Load) vs ELT (Extract-Load-Transform)**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                #### ETL Traditionnel
                **Processus:**
                1. **Extract** - Sources bases de donn√©es
                2. **Transform** - Serveur ETL d√©di√©
                3. **Load** - Data Warehouse
                
                **Avantages:**
                - Donn√©es propres charg√©es
                - Performance DW optimis√©e
                - Contr√¥le qualit√© strict
                
                **Outils:**
                - Informatica PowerCenter
                - IBM DataStage
                - Talend
                - SSIS
                """)
            
            with col2:
                st.markdown("""
                #### ELT Moderne
                **Processus:**
                1. **Extract** - Sources vari√©es
                2. **Load** - Data Lake brut
                3. **Transform** - Dans le cloud
                
                **Avantages:**
                - Flexibilit√© des transformations
                - Conservation donn√©es brutes
                - Scalabilit√© cloud
                
                **Outils:**
                - Matillion
                - dbt (Data Build Tool)
                - Fivetran
                - Stitch Data
                """)
            
            # Comparaison ETL vs ELT
            st.subheader("üìä Comparaison ETL vs ELT")
            
            comparison_data = {
                'Aspect': ['Latence', 'Flexibilit√©', 'Co√ªt', 'Comp√©tences requises', 'Cas d usage'],
                'ETL': ['Heures-jours', 'Limit√©e', '√âlev√© (serveurs d√©di√©s)', 'Sp√©cialis√©s ETL', 'Data Warehouse traditionnel'],
                'ELT': ['Minutes-heures', '√âlev√©e', 'Variable (cloud)', 'SQL + Scripting', 'Data Lake moderne']
            }
            
            df_comparison = pd.DataFrame(comparison_data)
            st.dataframe(df_comparison)
            
            # Exemple de pipeline ELT moderne
            st.subheader("üöÄ Exemple de Pipeline ELT avec dbt + Snowflake")
            
            code = """
-- models/mart/daily_sales.sql
{{ config(materialized='table') }}

WITH raw_sales AS (
    SELECT * FROM {{ source('raw', 'sales') }}
),

cleaned_sales AS (
    SELECT
        order_id,
        customer_id,
        product_id,
        amount,
        order_date,
        -- Nettoyage des donn√©es
        CASE 
            WHEN amount < 0 THEN 0 
            ELSE amount 
        END as cleaned_amount
    FROM raw_sales
    WHERE order_date >= '2024-01-01'
)

SELECT
    DATE_TRUNC('day', order_date) as sales_date,
    customer_id,
    SUM(cleaned_amount) as daily_revenue,
    COUNT(*) as number_orders
FROM cleaned_sales
GROUP BY 1, 2
            """
            st.code(code, language='sql')
            
            st.write("**Avantages de cette approche:**")
            st.write("- ‚úÖ Versioning des transformations avec Git")
            st.write("- ‚úÖ Tests de qualit√© des donn√©es int√©gr√©s")
            st.write("- ‚úÖ Documentation automatique")
            st.write("- ‚úÖ Lineage des donn√©es tra√ßable")
        
        elif integration_type == "CDC (Change Data Capture)":
            st.info("**üîÑ CDC - Capture des Changements en Temps R√©el**")
            
            st.markdown("""
            ### Principe du CDC
            
            Capture des modifications des bases de donn√©es sources en temps r√©el 
            sans impact sur les performances.
            """)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **M√©thodes CDC:**
                
                **1. Log-Based CDC**
                - Lecture des logs de transaction
                - Performances optimales
                - Exemple: Debezium, Oracle GoldenGate
                
                **2. Trigger-Based CDC**  
                - D√©clencheurs sur les tables
                - Impact sur la source
                - Exemple: Custom triggers
                
                **3. Query-Based CDC**
                - Polling r√©gulier avec timestamps
                - Simple √† impl√©menter
                - Latence plus √©lev√©e
                """)
            
            with col2:
                st.markdown("""
                **Cas d'Usage:**
                
                - üè¶ **R√©plication base de donn√©es** - Synchronisation cross-DC
                - üìä **Data Warehouse temps r√©el** - M√†j continue
                - üîç **Search Indexing** - Indexation recherche
                - üö® **Fraud Detection** - Surveillance transactions
                - üì± **Caching** - Invalidation caches
                """)
                
                # M√©triques CDC
                st.metric("Latence typique", "< 1 seconde")
                st.metric("Throughput", "10K+ events/sec")
                st.metric("Impact source", "N√©gligeable")
            
            # Architecture CDC avec Debezium + Kafka
            st.subheader("üèóÔ∏è Architecture CDC avec Debezium + Kafka")
            
            st.image("https://via.placeholder.com/700x300/2E8B57/FFFFFF?text=Debezium+%2B+Kafka+CDC+Pipeline", 
                    caption="Source DB ‚Üí Debezium ‚Üí Kafka ‚Üí Data Lake / Data Warehouse")
            
            # Exemple de configuration Debezium
            st.subheader("‚öôÔ∏è Configuration Debezium")
            
            code = """
# connector-config.json
{
  "name": "inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "database.server.name": "dbserver1",
    "database.include.list": "inventory",
    "table.include.list": "inventory.orders,inventory.customers",
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "dbhistory.inventory",
    "include.schema.changes": "true"
  }
}
            """
            st.code(code, language='json')
            
            st.write("**Flux de donn√©es captur√©es:**")
            st.write("- INSERT ‚Üí Nouveaux enregistrements")
            st.write("- UPDATE ‚Üí Anciennes et nouvelles valeurs")
            st.write("- DELETE ‚Üí Enregistrements supprim√©s")
        
        elif integration_type == "API Integration":
            st.warning("**üåê Int√©gration par APIs RESTful**")
            
            st.markdown("""
            ### Int√©gration de Donn√©es via APIs
            
            Connexion √† des sources externes via des APIs REST, GraphQL, SOAP.
            """)
            
            # Patterns d'int√©gration API
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **Patterns Courants:**
                
                **1. API Polling**
                - Requ√™tes r√©guli√®res
                - Simple √† impl√©menter
                - Latence variable
                
                **2. Webhooks**
                - Notifications push
                - Temps r√©el
                - Complexit√© c√¥t√© source
                
                **3. GraphQL**
                - Requ√™tes sp√©cifiques
                - R√©duction over-fetching
                - Courbe d'apprentissage
                """)
            
            with col2:
                st.markdown("""
                **Outils d'Orchestration:**
                
                - **Apache NiFi** - Flow-based programming
                - **Airbyte** - Connectors pr√©-built
                - **MuleSoft** - Platform enterprise
                - **Zapier** - No-code integration
                - **Custom Scripts** - Python, Node.js
                """)
                
                # Statistiques APIs
                st.metric("APIs disponibles", "50,000+")
                st.metric("Croissance march√©", "+25%/an")
                st.metric("Donn√©es via APIs", "60% des entreprises")
            
            # Exemple d'int√©gration API avec Python
            st.subheader("üêç Exemple d'Int√©gration API avec Python")
            
            code = """
import requests
import pandas as pd
from datetime import datetime, timedelta

class SalesforceAPI:
    def __init__(self, client_id, client_secret):
        self.base_url = "https://yourinstance.salesforce.com"
        self.access_token = self.authenticate(client_id, client_secret)
    
    def authenticate(self, client_id, client_secret):
        # OAuth2 authentication
        auth_url = f"{self.base_url}/services/oauth2/token"
        response = requests.post(auth_url, data={
            'grant_type': 'client_credentials',
            'client_id': client_id,
            'client_secret': client_secret
        })
        return response.json()['access_token']
    
    def extract_opportunities(self, start_date):
        headers = {'Authorization': f'Bearer {self.access_token}'}
        query = f'''
        SELECT Id, Name, Amount, CloseDate, StageName 
        FROM Opportunity 
        WHERE CloseDate >= {start_date}
        '''
        
        response = requests.get(
            f"{self.base_url}/services/data/v52.0/query",
            headers=headers,
            params={'q': query}
        )
        
        return pd.DataFrame(response.json()['records'])

# Utilisation
sf = SalesforceAPI("your-client-id", "your-client-secret")
opportunities = sf.extract_opportunities("2024-01-01")
            """
            st.code(code, language='python')
            
            # Bonnes pratiques
            st.subheader("üîß Bonnes Pratiques API Integration")
            
            practices = {
                "Rate Limiting": "Impl√©menter des retry avec backoff exponentiel",
                "Error Handling": "G√©rer les timeouts et erreurs HTTP",
                "Monitoring": "Tracker les quotas et latences",
                "Security": "Utiliser OAuth2, stocker secrets s√©curis√©ments",
                "Idempotence": "Assurer le re-processing sans duplication"
            }
            
            for practice, description in practices.items():
                with st.expander(f"‚úÖ {practice}"):
                    st.write(description)
        
        elif integration_type == "Data Virtualization":
            st.error("**üîÆ Virtualisation des Donn√©es**")
            
            st.markdown("""
            ### Acc√®s Unifi√© sans R√©plication
            
            Fournit une vue unifi√©e des donn√©es sans les d√©placer physiquement.
            """)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **Avantages:**
                - üöÄ **Time-to-value rapide** - Pas d'ETL n√©cessaire
                - üí∞ **Co√ªts r√©duits** - Pas de stockage suppl√©mentaire
                - üîí **Governance centralis√©e** - Contr√¥le d'acc√®s unique
                - üìä **Vue unifi√©e** - Multi-sources transparent
                
                **Inconv√©nients:**
                - ‚ö†Ô∏è **Performance** - D√©pend des sources
                - üîß **Complexit√©** - Optimisation des requ√™tes
                - üí∏ **Co√ªts licences** - Outils sp√©cialis√©s
                """)
            
            with col2:
                st.markdown("""
                **Outils:**
                - **Denodo** - Leader du march√©
                - **Dremio** - Open source friendly
                - **Starburst** - Bas√© sur Trino
                - **CData** - Connecteurs √©tendus
                
                **Cas d'Usage:**
                - üîç **Data Discovery** - Exploration sans ETL
                - üìà **Reporting ad-hoc** - Requ√™tes cross-sources
                - üè¶ **Compliance** - Vue 360¬∞ r√©glementaire
                - üîó **API Data Services** - Exposition donn√©es
                """)
            
            # Architecture Data Virtualization
            st.subheader("üèóÔ∏è Architecture de Virtualisation")
            
            st.image("https://via.placeholder.com/600x400/8B4513/FFFFFF?text=Data+Virtualization+Layer", 
                    caption="Sources multiples ‚Üí Virtualization Layer ‚Üí Consommateurs")
            
            # Exemple de requ√™te virtualis√©e
            st.subheader("üíª Exemple de Requ√™te Virtualis√©e")
            
            code = """
-- Requ√™te unifiant donn√©es Salesforce, MySQL et S3
SELECT 
    c.company_name,
    o.opportunity_amount,
    s.sentiment_score,
    DATE(o.created_date) as opportunity_date
FROM salesforce.opportunities o
JOIN mysql.customers c ON o.customer_id = c.id
LEFT JOIN s3.customer_feedback s ON c.email = s.customer_email
WHERE o.created_date >= '2024-01-01'
  AND o.status = 'Closed Won'
ORDER BY o.opportunity_amount DESC
LIMIT 100;
            """
            st.code(code, language='sql')
            
            st.write("**B√©n√©fices de cette approche:**")
            st.write("- ‚úÖ Pas de mouvement de donn√©es")
            st.write("- ‚úÖ Donn√©es toujours √† jour")
            st.write("- ‚úÖ Vue business unifi√©e")
            st.write("- ‚úÖ R√©duction time-to-insight")
        
        elif integration_type == "Data Replication":
            st.success("**üìã R√©plication de Donn√©es**")
            
            st.markdown("""
            ### Synchronisation des Donn√©es entre Syst√®mes
            
            Copie et synchronisation des donn√©es entre diff√©rentes bases et syst√®mes.
            """)
            
            # Types de r√©plication
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **Types de R√©plication:**
                
                **1. Snapshot Replication**
                - Copie compl√®te p√©riodique
                - Simple mais lourd
                - Pour donn√©es petites/moyennes
                
                **2. Transactional Replication**
                - Propagation des transactions
                - Faible latence
                - Pour synchronisation temps r√©el
                
                **3. Merge Replication**
                - Bidirectionnel
                - R√©solution conflits
                - Pour applications distribu√©es
                """)
            
            with col2:
                st.markdown("""
                **Outils Sp√©cialis√©s:**
                
                - **AWS DMS** - Database Migration Service
                - **Azure Data Factory** - Copy activities
                - **Google Dataflow** - Streaming replication
                - **Confluent** - Kafka-based replication
                - **Striim** - Real-time data integration
                """)
                
                # M√©triques r√©plication
                st.metric("Latence cible", "< 1 minute")
                st.metric("RTO (Recovery Time)", "Quelques minutes")
                st.metric("RPO (Recovery Point)", "Quelques secondes")
            
            # Exemple AWS DMS
            st.subheader("‚òÅÔ∏è Exemple AWS Database Migration Service")
            
            code = """
# Configuration de t√¢che DMS avec CloudFormation
AWSTemplateFormatVersion: '2010-09-09'
Resources:
  ReplicationTask:
    Type: AWS::DMS::ReplicationTask
    Properties:
      ReplicationTaskIdentifier: "mysql-to-redshift"
      SourceEndpointArn: !Ref SourceEndpoint
      TargetEndpointArn: !Ref TargetEndpoint
      ReplicationInstanceArn: !Ref ReplicationInstance
      MigrationType: "full-load-and-cdc"
      TableMappings: |
        {
          "rules": [
            {
              "rule-type": "selection",
              "rule-id": "1",
              "rule-name": "1",
              "object-locator": {
                "schema-name": "ecommerce",
                "table-name": "%"
              },
              "rule-action": "include"
            }
          ]
        }
            """
            st.code(code, language='yaml')
            
            # Cas d'usage r√©plication
            st.subheader("üéØ Cas d'Usage Typiques")
            
            use_cases = {
                "üè¢ **Migration Cloud**": "Transfert on-premise ‚Üí cloud",
                "üîÑ **Disaster Recovery**": "R√©plication cross-region",
                "üìä **Analytics Separation**": "Copie prod ‚Üí analytics",
                "üåç **Global Applications**": "Synchronisation multi-regions"
            }
            
            for use_case, description in use_cases.items():
                st.write(f"{use_case}: {description}")
        
             # Tableau comparatif g√©n√©ral - AVEC INDICATEURS VISUELS
        st.subheader("üìä Comparatif des M√©thodes d'Int√©gration")
        
        # Cr√©er des indicateurs visuels
        comparison_data = {
            'M√©thode': ['ETL/ELT', 'CDC', 'API Integration', 'Data Virtualization', 'Data Replication'],
            'Latence': ['üî¥ Heures', 'üü¢ Secondes', 'üü° Minutes', 'üü¢ Secondes', 'üü° Minutes'],
            'Complexit√©': ['üî¥ √âlev√©e', 'üü¢ Moyenne', 'üü° Variable', 'üî¥ √âlev√©e', 'üü¢ Moyenne'],
            'Co√ªt': ['üî¥ √âlev√©', 'üü¢ Moyen', 'üü° Variable', 'üî¥ √âlev√©', 'üü¢ Moyen'],
            'Temps R√©el': ['üî¥ Non', 'üü¢ Oui', 'üü° Partiel', 'üü¢ Oui', 'üü¢ Oui'],
            'Use Case': ['Data Warehousing', 'Temps r√©el', 'SaaS Integration', 'Vue unifi√©e', 'Synchronisation']
        }
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison)
        
 

# =============================================================================
# SECTION 3: CAS PRATIQUES PAR SECTEUR
# =============================================================================


def generer_donnees_sante():
    """G√©n√®re des donn√©es r√©alistes pour le secteur sant√©"""
    np.random.seed(42)
    
    # Donn√©es patients
    n_patients = 500
    data_patients = {
        'patient_id': range(1, n_patients + 1),
        'age': np.random.randint(18, 90, n_patients),
        'sexe': np.random.choice(['M', 'F'], n_patients),
        'poids': np.random.normal(70, 15, n_patients),
        'taille': np.random.normal(170, 10, n_patients),
        'tension_arterielle': np.random.normal(120, 20, n_patients),
        'cholesterol': np.random.normal(180, 40, n_patients),
        'diabete': np.random.choice([0, 1], n_patients, p=[0.85, 0.15]),
        'fumeur': np.random.choice([0, 1], n_patients, p=[0.7, 0.3]),
        'region': np.random.choice(['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor'], n_patients)
    }
    
    df_patients = pd.DataFrame(data_patients)
    df_patients['poids'] = df_patients['poids'].clip(40, 150)
    df_patients['taille'] = df_patients['taille'].clip(150, 200)
    df_patients['tension_arterielle'] = df_patients['tension_arterielle'].clip(80, 200)
    df_patients['cholesterol'] = df_patients['cholesterol'].clip(100, 300)
    
    # Donn√©es consultations
    consultations = []
    for patient_id in range(1, n_patients + 1):
        n_consultations = np.random.poisson(3) + 1  # Au moins 1 consultation
        for _ in range(n_consultations):
            consultations.append({
                'patient_id': patient_id,
                'date': datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 365)),
                'medecin': np.random.choice(['Dr. Diallo', 'Dr. Ndiaye', 'Dr. Sarr', 'Dr. Diop', 'Dr. Fall']),
                'diagnostic_principal': np.random.choice(['Hypertension', 'Diab√®te', 'Grippe', 'Arthrite', 'Asthme', 'Anxi√©t√©', 'D√©pression']),
                'duree_minutes': np.random.randint(15, 60),
                'cout': np.random.normal(50, 20),
                'medicaments_prescrits': np.random.randint(0, 5)
            })
    
    df_consultations = pd.DataFrame(consultations)
    df_consultations['cout'] = df_consultations['cout'].clip(20, 150)
    
    return df_patients, df_consultations

def generer_donnees_telecom():
    """G√©n√®re des donn√©es r√©alistes pour le secteur t√©l√©com"""
    np.random.seed(42)
    
    n_clients = 1000
    data_telecom = {
        'client_id': range(1, n_clients + 1),
        'forfait': np.random.choice(['Basique', 'Standard', 'Premium', 'Illimit√©'], n_clients, p=[0.3, 0.4, 0.2, 0.1]),
        'anciennete_mois': np.random.randint(1, 60, n_clients),
        'data_utilisee_go': np.random.gamma(2, 10, n_clients),
        'appels_minutes': np.random.gamma(3, 50, n_clients),
        'sms_envoyes': np.random.poisson(30, n_clients),
        'facture_moyenne': np.random.normal(25, 10, n_clients),
        'satisfaction': np.random.randint(1, 11, n_clients),
        'region': np.random.choice(['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor'], n_clients),
        'nb_reclamations': np.random.poisson(0.5, n_clients)
    }
    
    df_telecom = pd.DataFrame(data_telecom)
    df_telecom['data_utilisee_go'] = df_telecom['data_utilisee_go'].clip(0, 100)
    df_telecom['appels_minutes'] = df_telecom['appels_minutes'].clip(0, 1000)
    df_telecom['facture_moyenne'] = df_telecom['facture_moyenne'].clip(10, 80)
    df_telecom['nb_reclamations'] = df_telecom['nb_reclamations'].clip(0, 5)
    
    return df_telecom

def section_cas_pratiques():
    st.header("üè¢ Cas Pratiques par Secteur d'Activit√©")
    
    secteur = st.selectbox(
        "Choisir un secteur:",
        ["Commerce & Retail", "Finance & Assurance", "Sant√©", "T√©l√©com", "Transport & Logistique", "√ânergie"]
    )
    
    if secteur == "Commerce & Retail":
        st.subheader("üõí Commerce & Retail")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### Applications Big Data
            
            **1. Recommandation Personnalis√©e**
            - Analyse comportement client en temps r√©el
            - Algorithmes de collaborative filtering
            - Boost du panier moyen de 15-30%
            
            **2. Optimisation des Stocks**
            - Pr√©vision demande saisonni√®re
            - Analyse des tendances ventes
            - R√©duction rupture de stock de 40%
            
            **3. Pricing Dynamique**
            - Analyse concurrentielle en temps r√©el
            - Elasticit√© prix/demande
            - Maximisation marge de 8-12%
            
            **4. Analyse du Parcours Client**
            - Tracking multi-canal
            - Optimisation de l'exp√©rience client
            - Augmentation conversion de 20-35%
            """)
        
        with col2:
            # Simulation donn√©es retail
            df_clients, df_transactions = generer_donnees_retail()
            
            # KPI principaux
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Clients totaux", f"{len(df_clients):,}")
            with col2:
                st.metric("Transactions totales", f"{len(df_transactions):,}")
            with col3:
                st.metric("Panier moyen", f"{df_transactions['montant'].mean():.1f} ‚Ç¨")
            
            # Analyse des ventes par cat√©gorie
            ventes_par_categorie = df_transactions.groupby('categorie')['montant'].sum().sort_values(ascending=False)
            fig = px.pie(ventes_par_categorie, values=ventes_par_categorie.values, names=ventes_par_categorie.index,
                        title="R√©partition du Chiffre d'Affaires par Cat√©gorie")
            st.plotly_chart(fig, use_container_width=True)
            
            # Performance des canaux de vente
            perf_canal = df_transactions.groupby('canal').agg({
                'montant': ['count', 'mean', 'sum']
            }).round(2)
            perf_canal.columns = ['Nb Transactions', 'Panier Moyen', 'CA Total']
            st.write("**Performance par canal de vente:**")
            st.dataframe(perf_canal)
    
    elif secteur == "Finance & Assurance":
        st.subheader("üè¶ Finance & Assurance")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### Applications Big Data
            
            **1. D√©tection de Fraude**
            - Analyse patterns transactionnels
            - Machine Learning temps r√©el
            - R√©duction fraude de 60-80%
            
            **2. Scoring Cr√©dit**
            - Analyse comportementale avanc√©e
            - Donn√©es alternatives
            - Pr√©cision am√©lior√©e de 25%
            
            **3. Conformit√© R√©glementaire**
            - Surveillance transactions
            - Reporting automatique
            - R√©duction co√ªts compliance de 40%
            
            **4. Gestion des Risques**
            - Analyse de portefeuille
            - Stress testing
            - Optimisation capital r√©glementaire
            """)
        
        with col2:
            # Simulation d√©tection fraude
            np.random.seed(42)
            transactions_data = {
                'Heure': list(range(24)) * 30,
                'Montant': np.random.exponential(100, 720),
                'Type': np.random.choice(['Retrait', 'Paiement', 'Virement'], 720, p=[0.3, 0.5, 0.2]),
                'Risque_Fraude': np.random.beta(0.5, 5, 720)
            }
            
            df_transactions = pd.DataFrame(transactions_data)
            df_transactions['Fraude'] = df_transactions['Risque_Fraude'] > 0.8
            
            # Transactions par heure avec d√©tection fraude
            transactions_par_heure = df_transactions.groupby('Heure').agg({
                'Montant': 'count',
                'Fraude': 'sum'
            }).reset_index()
            
            fig = px.line(transactions_par_heure, x='Heure', y=['Montant', 'Fraude'],
                         title="Activit√© Transactionnelle et Fraudes sur 24h",
                         labels={'value': 'Nombre', 'variable': 'Type'})
            st.plotly_chart(fig, use_container_width=True)
            
            # M√©triques de fraude
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Transactions analys√©es", f"{len(df_transactions):,}")
            with col2:
                st.metric("Fraudes d√©tect√©es", f"{df_transactions['Fraude'].sum()}")
            with col3:
                taux_fraude = (df_transactions['Fraude'].sum() / len(df_transactions)) * 100
                st.metric("Taux de fraude", f"{taux_fraude:.2f}%")
    
    elif secteur == "Sant√©":
        st.success("**üè• Secteur Sant√© - Applications Big Data**")
        
        df_patients, df_consultations = generer_donnees_sante()
        
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä Tableau de Bord Sant√©", "üîç Analytics Pr√©dictif", 
            "üíä Gestion des Ressources", "üè• T√©l√©m√©decine"
        ])
        
        with tab1:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Nombre de patients", f"{len(df_patients):,}")
                st.metric("Taux de diab√®te", f"{(df_patients['diabete'].mean() * 100):.1f}%")
            
            with col2:
                st.metric("Consultations totales", f"{len(df_consultations):,}")
                st.metric("Co√ªt moyen consultation", f"{df_consultations['cout'].mean():.1f} ‚Ç¨")
            
            with col3:
                age_moyen = df_patients['age'].mean()
                st.metric("√Çge moyen patients", f"{age_moyen:.1f} ans")
                st.metric("Taux de fumeurs", f"{(df_patients['fumeur'].mean() * 100):.1f}%")
            
            # Visualisations sant√©
            col1, col2 = st.columns(2)
            
            with col1:
                # Distribution des diagnostics
                diag_counts = df_consultations['diagnostic_principal'].value_counts().head(10)
                fig = px.pie(diag_counts, values=diag_counts.values, names=diag_counts.index,
                           title="Top 10 des diagnostics")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Tension art√©rielle par √¢ge
                fig = px.scatter(df_patients, x='age', y='tension_arterielle', 
                               color='diabete', size='cholesterol',
                               title="Tension art√©rielle vs √Çge",
                               hover_data=['sexe', 'fumeur'])
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.subheader("üîÆ Pr√©diction des Risques Sant√©")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Calculateur de risque cardio-vasculaire
                st.write("**Calculateur de Risque Cardio-Vasculaire**")
                
                age = st.slider("√Çge", 20, 80, 45)
                tension = st.slider("Tension art√©rielle", 80, 200, 120)
                cholesterol = st.slider("Cholest√©rol", 100, 300, 180)
                fumeur = st.checkbox("Fumeur")
                diabete = st.checkbox("Diab√©tique")
                sexe = st.selectbox("Sexe", ["Homme", "Femme"])
                
                # Calcul simplifi√© du risque (mod√®le FACTICE pour d√©monstration)
                risque_base = 5
                if sexe == "Homme":
                    risque_base += 10
                
                risque = (risque_base + 
                         age * 0.5 + 
                         max(0, tension - 120) * 0.2 + 
                         max(0, cholesterol - 180) * 0.1 + 
                         fumeur * 15 + 
                         diabete * 20)
                risque = min(100, max(5, risque))
                
                st.metric("Risque cardio-vasculaire", f"{risque:.1f}%")
                
                if risque > 70:
                    st.error("Risque √âLEV√â - Consultation m√©dicale recommand√©e")
                elif risque > 40:
                    st.warning("Risque MOD√âR√â - Surveillance recommand√©e")
                else:
                    st.success("Risque FAIBLE - Maintenir mode de vie sain")
            
            with col2:
                # Analyse des co√ªts de sant√©
                couts_par_diagnostic = df_consultations.groupby('diagnostic_principal')['cout'].mean().sort_values(ascending=False).head(10)
                fig = px.bar(couts_par_diagnostic, 
                           title="Co√ªt moyen par diagnostic (Top 10)",
                           labels={'value': 'Co√ªt moyen (‚Ç¨)', 'diagnostic_principal': 'Diagnostic'})
                st.plotly_chart(fig, use_container_width=True)
                
                # Distribution des dur√©es de consultation
                fig = px.histogram(df_consultations, x='duree_minutes', 
                                 title="Distribution des dur√©es de consultation",
                                 nbins=20)
                st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            st.subheader("üìà Optimisation des Ressources M√©dicales")
            
            # Analyse de la charge de travail
            charge_medecins = df_consultations.groupby('medecin').agg({
                'patient_id': 'count',
                'duree_minutes': 'sum',
                'cout': 'sum'
            }).round(2).sort_values('patient_id', ascending=False)
            
            charge_medecins.columns = ['Nb Patients', 'Total Minutes', 'Total Co√ªt']
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Charge de travail par m√©decin**")
                st.dataframe(charge_medecins)
            
            with col2:
                fig = px.bar(charge_medecins, y='Nb Patients',
                           title="Nombre de patients par m√©decin")
                st.plotly_chart(fig, use_container_width=True)
            
            # Efficacit√© des m√©decins
            charge_medecins['Patients par Heure'] = (charge_medecins['Nb Patients'] / (charge_medecins['Total Minutes'] / 60)).round(2)
            charge_medecins['Co√ªt par Patient'] = (charge_medecins['Total Co√ªt'] / charge_medecins['Nb Patients']).round(2)
            
            st.write("**Indicateurs d'efficacit√©:**")
            st.dataframe(charge_medecins[['Patients par Heure', 'Co√ªt par Patient']])
            
            # Recommandations d'optimisation
            st.subheader("üí° Recommandations d'Am√©lioration")
            st.write("""
            - **R√©√©quilibrer la charge** entre m√©decins surcharg√©s et moins occup√©s
            - **Optimiser les plannings** bas√© sur la dur√©e moyenne des consultations
            - **D√©velopper la t√©l√©m√©decine** pour les suivis simples
            - **Automatiser** les t√¢ches administratives r√©p√©titives
            - **Formation continue** sur les diagnostics les plus fr√©quents
            """)
        
        with tab4:
            st.subheader("üè• T√©l√©m√©decine et Innovation")
            
            st.markdown("""
            ### Applications Big Data en T√©l√©m√©decine
            
            **1. Surveillance √† Distance**
            - Monitoring des patients chroniques
            - Alertes pr√©coces pour les complications
            - R√©duction des hospitalisations de 30%
            
            **2. Diagnostic Assist√©**
            - IA pour l'analyse d'imagerie m√©dicale
            - Algorithmes de diagnostic diff√©rentiel
            - Am√©lioration pr√©cision diagnostic de 25%
            
            **3. Recherche Clinique**
            - Analyse de donn√©es patients √† grande √©chelle
            - Identification de nouveaux traitements
            - Acc√©l√©ration des d√©couvertes m√©dicales
            
            **4. Pr√©vention Personnalis√©e**
            - Recommandations sant√© individualis√©es
            - D√©tection pr√©coce des risques
            - Am√©lioration de la sant√© populationnelle
            """)
            
            # Simulation d'impact t√©l√©m√©decine
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Consultations √† distance", "35%", "+15% vs 2023")
            
            with col2:
                st.metric("√âconomies r√©alis√©es", "2.5M ‚Ç¨", "+28%")
            
            with col3:
                st.metric("Satisfaction patients", "4.5/5", "+0.7 pts")
    
    elif secteur == "T√©l√©com":
        st.info("**üìû Secteur T√©l√©com - Applications Big Data**")
        
        df_telecom = generer_donnees_telecom()
        
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä Performance R√©seau", "üì± Analyse Clients", 
            "üéØ Marketing Cibl√©", "üö® Pr√©vention Fraude"
        ])
        
        with tab1:
            st.subheader("Performance du R√©seau et Satisfaction Client")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Clients totaux", f"{len(df_telecom):,}")
                st.metric("Satisfaction moyenne", f"{df_telecom['satisfaction'].mean():.1f}/10")
            
            with col2:
                data_moyenne = df_telecom['data_utilisee_go'].mean()
                st.metric("Data utilis√©e moyenne", f"{data_moyenne:.1f} Go")
                st.metric("Facture moyenne", f"{df_telecom['facture_moyenne'].mean():.1f} ‚Ç¨")
            
            with col3:
                appels_moyens = df_telecom['appels_minutes'].mean()
                st.metric("Appels moyens/mois", f"{appels_moyens:.0f} min")
                st.metric("SMS moyens/mois", f"{df_telecom['sms_envoyes'].mean():.0f}")
            
            # Analyse par forfait
            col1, col2 = st.columns(2)
            
            with col1:
                performance_forfait = df_telecom.groupby('forfait').agg({
                    'satisfaction': 'mean',
                    'data_utilisee_go': 'mean',
                    'facture_moyenne': 'mean',
                    'nb_reclamations': 'mean'
                }).round(2)
                
                st.write("**Performance par type de forfait**")
                st.dataframe(performance_forfait)
            
            with col2:
                fig = px.box(df_telecom, x='forfait', y='satisfaction',
                           title="Satisfaction client par forfait")
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.subheader("üì± Analyse Comportementale des Clients")
            
            # Segmentation des clients
            df_telecom['utilisation_totale'] = (df_telecom['data_utilisee_go'] + 
                                              df_telecom['appels_minutes']/100 + 
                                              df_telecom['sms_envoyes']/10)
            
            # Clustering des clients
            features = ['data_utilisee_go', 'appels_minutes', 'sms_envoyes', 'facture_moyenne']
            X = df_telecom[features]
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            kmeans = KMeans(n_clusters=4, random_state=42)
            df_telecom['segment'] = kmeans.fit_predict(X_scaled)
            
            segment_descriptions = {
                0: "üìä Faibles Utilisateurs - Co√ªt faible",
                1: "üì± Utilisateurs Data - Forte consommation data",
                2: "üìû Utilisateurs Voix - Forte consommation appels", 
                3: "üíé Power Users - Forte consommation globale"
            }
            
            col1, col2 = st.columns(2)
            
            with col1:
                segment_counts = df_telecom['segment'].value_counts().sort_index()
                segments_named = [segment_descriptions[i] for i in segment_counts.index]
                
                fig = px.pie(values=segment_counts.values, names=segments_named,
                           title="Segmentation des clients")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("üìã Profils des Segments")
                for segment_id, description in segment_descriptions.items():
                    segment_data = df_telecom[df_telecom['segment'] == segment_id]
                    count = len(segment_data)
                    satisfaction = segment_data['satisfaction'].mean()
                    facture_moyenne = segment_data['facture_moyenne'].mean()
                    
                    st.write(f"**{description}** ({count} clients)")
                    st.write(f"- Satisfaction: {satisfaction:.1f}/10")
                    st.write(f"- Facture moyenne: {facture_moyenne:.1f} ‚Ç¨")
                    st.write(f"- Data moyenne: {segment_data['data_utilisee_go'].mean():.1f} Go")
                    st.write("---")
        
        with tab3:
            st.subheader("üéØ Marketing et Fid√©lisation Cibl√©s")
            
            # Pr√©diction de d√©sabonnement (churn)
            df_telecom['score_churn'] = np.random.beta(2, 5, len(df_telecom))
            df_telecom['risque_churn'] = pd.cut(df_telecom['score_churn'], 
                                              bins=[0, 0.3, 0.7, 1], 
                                              labels=['Faible', 'Moyen', '√âlev√©'])
            
            col1, col2 = st.columns(2)
            
            with col1:
                risque_churn_count = df_telecom['risque_churn'].value_counts()
                fig = px.bar(risque_churn_count, 
                           title="R√©partition du risque de d√©sabonnement",
                           labels={'value': 'Nombre clients', 'risque_churn': 'Risque'})
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("üí° Strat√©gies de Fid√©lisation")
                
                strategies = {
                    'Faible': """
                    ‚úÖ **Maintenir la qualit√© de service**
                    - Offres de fid√©lit√©
                    - Programme de parrainage
                    - Communications personnalis√©es
                    """,
                    'Moyen': """
                    üìû **Contact proactif**
                    - Offres personnalis√©es
                    - Enqu√™te satisfaction
                    - Service client d√©di√©
                    """, 
                    '√âlev√©': """
                    üö® **Intervention imm√©diate**
                    - Offres promotionnelles
                    - Service premium
                    - Analyse des causes de m√©contentement
                    """
                }
                
                for risque, strategie in strategies.items():
                    count = len(df_telecom[df_telecom['risque_churn'] == risque])
                    st.write(f"**Risque {risque}** ({count} clients)")
                    st.write(strategie)
                    st.write("")
        
        with tab4:
            st.subheader("üö® D√©tection de Fraude et S√©curit√©")
            
            # Simulation de d√©tection d'anomalies
            st.write("**D√©tection des comportements anormaux**")
            
            # Seuils pour d√©tection d'anomalies (percentile 95)
            seuil_data = df_telecom['data_utilisee_go'].quantile(0.95)
            seuil_appels = df_telecom['appels_minutes'].quantile(0.95)
            seuil_sms = df_telecom['sms_envoyes'].quantile(0.95)
            
            anomalies = df_telecom[
                (df_telecom['data_utilisee_go'] > seuil_data) | 
                (df_telecom['appels_minutes'] > seuil_appels) |
                (df_telecom['sms_envoyes'] > seuil_sms)
            ]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Seuil data anormale", f"{seuil_data:.1f} Go")
                st.metric("Seuil appels anormaux", f"{seuil_appels:.0f} min")
                st.metric("Seuil SMS anormaux", f"{seuil_sms:.0f}")
                st.metric("Clients suspects d√©tect√©s", f"{len(anomalies)}")
            
            with col2:
                if len(anomalies) > 0:
                    st.write("**Top 5 comportements suspects**")
                    suspects_display = anomalies[['client_id', 'data_utilisee_go', 'appels_minutes', 'sms_envoyes', 'forfait']].head()
                    st.dataframe(suspects_display.style.format({
                        'data_utilisee_go': '{:.1f}',
                        'appels_minutes': '{:.0f}',
                        'sms_envoyes': '{:.0f}'
                    }))
            
            # Recommandations anti-fraude
            st.subheader("üõ°Ô∏è Mesures de Pr√©vention")
            st.write("""
            - **Surveillance en temps r√©el** de la consommation
            - **Alertes automatiques** pour les d√©passements de seuils
            - **V√©rification d'identit√©** renforc√©e pour les forfaits haut de gamme
            - **Analyse des patterns** de consommation inhabituels
            - **Collaboration inter-op√©rateurs** pour d√©tecter les fraudes coordonn√©es
            - **Blocage automatique** des activit√©s suspectes
            - **Audit r√©gulier** des syst√®mes de s√©curit√©
            """)
    
    elif secteur == "Transport & Logistique":
        st.warning("**üöö Transport & Logistique - Applications Big Data**")
        
        st.subheader("Optimisation de la Supply Chain")
        
        # Simulation de donn√©es logistiques
        np.random.seed(42)
        n_livraisons = 500
        
        data_logistique = {
            'livraison_id': range(1, n_livraisons + 1),
            'date': [datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(n_livraisons)],
            'region': np.random.choice(['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor'], n_livraisons),
            'poids_kg': np.random.uniform(1, 1000, n_livraisons),
            'distance_km': np.random.uniform(5, 500, n_livraisons),
            'duree_livraison_heures': np.random.uniform(1, 72, n_livraisons),
            'cout_livraison': np.random.uniform(10, 500, n_livraisons),
            'retard_heures': np.random.exponential(2, n_livraisons),
            'satisfaction_client': np.random.randint(1, 10, n_livraisons),
            'type_transport': np.random.choice(['Camion', 'Utilitaire', 'Moto'], n_livraisons, p=[0.6, 0.3, 0.1])
        }
        
        df_logistique = pd.DataFrame(data_logistique)
        df_logistique['retard_heures'] = df_logistique['retard_heures'].clip(0, 24)
        
        tab1, tab2, tab3 = st.tabs(["üì¶ Performance Livraison", "üöõ Optimisation Routes", "üìä Analytics Pr√©dictif"])
        
        with tab1:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                retard_moyen = df_logistique['retard_heures'].mean()
                st.metric("Retard moyen", f"{retard_moyen:.1f} h")
                taux_livraison_temps = (len(df_logistique[df_logistique['retard_heures'] < 1])/len(df_logistique)*100)
                st.metric("Taux de livraison √† temps", f"{taux_livraison_temps:.1f}%")
            
            with col2:
                cout_moyen = df_logistique['cout_livraison'].mean()
                st.metric("Co√ªt moyen livraison", f"{cout_moyen:.1f} ‚Ç¨")
                st.metric("Satisfaction moyenne", f"{df_logistique['satisfaction_client'].mean():.1f}/10")
            
            with col3:
                distance_moyenne = df_logistique['distance_km'].mean()
                st.metric("Distance moyenne", f"{distance_moyenne:.1f} km")
                st.metric("Poids moyen", f"{df_logistique['poids_kg'].mean():.1f} kg")
            
            # Analyse des performances par r√©gion
            col1, col2 = st.columns(2)
            
            with col1:
                perf_region = df_logistique.groupby('region').agg({
                    'retard_heures': 'mean',
                    'satisfaction_client': 'mean',
                    'cout_livraison': 'mean',
                    'livraison_id': 'count'
                }).round(2)
                
                perf_region.columns = ['Retard moyen (h)', 'Satisfaction', 'Co√ªt moyen (‚Ç¨)', 'Nb Livraisons']
                st.write("**Performance par r√©gion**")
                st.dataframe(perf_region)
            
            with col2:
                fig = px.scatter(df_logistique, x='distance_km', y='cout_livraison',
                               color='type_transport', size='poids_kg',
                               title="Co√ªt vs Distance par type de transport",
                               hover_data=['region'])
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.subheader("üöõ Optimisation des Itin√©raires")
            
            # Simulation d'optimisation de tourn√©es
            st.write("**Simulateur d'Optimisation de Tourn√©e**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                n_livraisons_tournee = st.slider("Nombre de livraisons", 5, 50, 20)
                capacite_vehicule = st.slider("Capacit√© v√©hicule (kg)", 500, 2000, 1000)
                zone_couverte = st.selectbox("Zone de livraison", ['Dakar', 'Thi√®s', 'R√©gionale'])
            
            with col2:
                traffic = st.select_slider("Niveau de trafic", options=['Faible', 'Moyen', '√âlev√©'])
                conditions_meteo = st.select_slider("Conditions m√©t√©o", options=['Bonnes', 'Moyennes', 'Mauvaises'])
            
            # Calcul d'optimisation simplifi√©
            if st.button("Optimiser la tourn√©e"):
                # Facteurs d'impact
                facteur_traffic = {'Faible': 1.0, 'Moyen': 1.3, '√âlev√©': 1.7}
                facteur_meteo = {'Bonnes': 1.0, 'Moyennes': 1.2, 'Mauvaises': 1.5}
                facteur_zone = {'Dakar': 1.0, 'Thi√®s': 1.1, 'R√©gionale': 1.4}
                
                duree_optimale = (n_livraisons_tournee * 0.5 * 
                                facteur_traffic[traffic] * 
                                facteur_meteo[conditions_meteo] * 
                                facteur_zone[zone_couverte])
                
                cout_optimise = (n_livraisons_tournee * 8 * 
                               facteur_traffic[traffic] * 
                               facteur_meteo[conditions_meteo] * 
                               facteur_zone[zone_couverte])
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Dur√©e estim√©e", f"{duree_optimale:.1f} h")
                    st.metric("Distance estim√©e", f"{(n_livraisons_tournee * 15 * facteur_zone[zone_couverte]):.0f} km")
                with col2:
                    st.metric("Co√ªt estim√©", f"{cout_optimise:.1f} ‚Ç¨")
                    st.metric("√âconomies potentielles", f"{(cout_optimise * 0.15):.1f} ‚Ç¨", "-15%")
                
                st.success("**Tourn√©e optimis√©e g√©n√©r√©e !**")
                st.write("""
                **Recommandations:**
                - Regrouper les livraisons par zone g√©ographique
                - Prioriser les livraisons urgentes en matin√©e
                - Adapter les v√©hicules au type de marchandise
                - Pr√©voir des itin√©raires alternatifs pour les zones √† fort trafic
                """)
        
        with tab3:
            st.subheader("üìä Analytics Pr√©dictif Logistique")
            
            # Pr√©diction des retards
            st.write("**Pr√©diction des Risques de Retard**")
            
            # Facteurs influen√ßant les retards (mod√®le simplifi√©)
            df_logistique['risque_retard'] = (
                df_logistique['distance_km'] * 0.001 +
                df_logistique['poids_kg'] * 0.0005 +
                np.random.normal(0, 0.1, len(df_logistique))
            )
            df_logistique['risque_retard'] = df_logistique['risque_retard'].clip(0, 1)
            df_logistique['niveau_risque'] = pd.cut(df_logistique['risque_retard'], 
                                                  bins=[0, 0.3, 0.7, 1], 
                                                  labels=['Faible', 'Moyen', '√âlev√©'])
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.histogram(df_logistique, x='risque_retard', 
                                 title="Distribution du risque de retard",
                                 nbins=20)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                risque_par_region = df_logistique.groupby('region')['risque_retard'].mean().sort_values(ascending=False)
                fig = px.bar(risque_par_region, 
                           title="Risque de retard moyen par r√©gion")
                st.plotly_chart(fig, use_container_width=True)
            
            # Calculateur de risque individuel
            st.subheader("üîÆ Calculateur de Risque de Retard")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                distance = st.slider("Distance (km)", 10, 500, 100)
                poids = st.slider("Poids (kg)", 1, 1000, 100)
            
            with col2:
                region = st.selectbox("R√©gion de destination", ['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor'])
                type_transp = st.selectbox("Type transport", ['Camion', 'Utilitaire', 'Moto'])
            
            with col3:
                jour_semaine = st.selectbox("Jour de livraison", ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi'])
                meteo = st.select_slider("Pr√©visions m√©t√©o", ['Bonnes', 'Moyennes', 'Mauvaises'])
            
            # Calcul du risque (mod√®le FACTICE pour d√©monstration)
            facteur_region = {'Dakar': 0.1, 'Thi√®s': 0.2, 'Diourbel': 0.3, 'Saint-Louis': 0.4, 'Ziguinchor': 0.5}
            facteur_transport = {'Camion': 0.1, 'Utilitaire': 0.2, 'Moto': 0.3}
            facteur_jour = {'Lundi': 0.1, 'Mardi': 0.1, 'Mercredi': 0.1, 'Jeudi': 0.1, 'Vendredi': 0.2, 'Samedi': 0.3}
            facteur_meteo_calc = {'Bonnes': 0.1, 'Moyennes': 0.2, 'Mauvaises': 0.4}
            
            risque_calcul√© = (
                distance * 0.001 +
                poids * 0.0005 +
                facteur_region[region] +
                facteur_transport[type_transp] +
                facteur_jour[jour_semaine] +
                facteur_meteo_calc[meteo]
            )
            risque_calcul√© = min(0.95, max(0.05, risque_calcul√©))
            
            st.metric("Risque de retard estim√©", f"{risque_calcul√©:.1%}")
            
            if risque_calcul√© > 0.7:
                st.error("‚ö†Ô∏è Risque √âLEV√â - Planifier des contingences")
                st.write("**Actions recommand√©es:** D√©part plus t√¥t, V√©hicule de remplacement, Itin√©raire alternatif")
            elif risque_calcul√© > 0.4:
                st.warning("üìã Risque MOD√âR√â - Surveillance recommand√©e")
                st.write("**Actions recommand√©es:** Surveillance GPS, Contact client pr√©ventif")
            else:
                st.success("‚úÖ Risque FAIBLE - Livraison normale")
                st.write("**Actions recommand√©es:** Suivi standard, Communication normale")
    
    elif secteur == "√ânergie":
        st.success("**‚ö° Secteur √ânergie - Applications Big Data**")
        
        st.subheader("Optimisation de la Production et Distribution")
        
        # Simulation de donn√©es √©nerg√©tiques
        np.random.seed(42)
        n_jours = 365
        dates = pd.date_range('2024-01-01', periods=n_jours, freq='D')
        
        data_energie = {
            'date': dates,
            'consommation_mwh': np.random.normal(1000, 200, n_jours) + 
                              np.sin(np.arange(n_jours) * 2 * np.pi / 365) * 100 +
                              np.sin(np.arange(n_jours) * 2 * np.pi / 7) * 50,
            'production_solaire': np.random.normal(300, 100, n_jours) * 
                                (1 + np.sin(np.arange(n_jours) * 2 * np.pi / 365) * 0.3),
            'production_eolienne': np.random.normal(200, 80, n_jours) * 
                                 (1 + np.cos(np.arange(n_jours) * 2 * np.pi / 365) * 0.4),
            'production_gaz': np.random.normal(500, 100, n_jours),
            'temperature': np.random.normal(25, 5, n_jours) + 
                          np.sin(np.arange(n_jours) * 2 * np.pi / 365) * 10,
            'prix_marche_‚Ç¨_mwh': np.random.normal(50, 15, n_jours),
            'pannes_reseau': np.random.poisson(0.1, n_jours)
        }
        
        df_energie = pd.DataFrame(data_energie)
        df_energie['production_totale'] = (df_energie['production_solaire'] + 
                                         df_energie['production_eolienne'] + 
                                         df_energie['production_gaz'])
        df_energie['equilibre_reseau'] = df_energie['production_totale'] - df_energie['consommation_mwh']
        
        tab1, tab2, tab3 = st.tabs(["üìà Production/Consommation", "üîß Maintenance Pr√©dictive", "üí∞ Optimisation March√©"])
        
        with tab1:
            st.subheader("√âquilibre Production-Consommation")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                conso_moyenne = df_energie['consommation_mwh'].mean()
                st.metric("Consommation moyenne", f"{conso_moyenne:.0f} MWh/jour")
                st.metric("Pic de consommation", f"{df_energie['consommation_mwh'].max():.0f} MWh")
            
            with col2:
                prod_moyenne = df_energie['production_totale'].mean()
                st.metric("Production moyenne", f"{prod_moyenne:.0f} MWh/jour")
                taux_couverture = (prod_moyenne/conso_moyenne*100)
                st.metric("Taux couverture", f"{taux_couverture:.1f}%")
            
            with col3:
                equilibre_moyen = df_energie['equilibre_reseau'].mean()
                st.metric("√âquilibre r√©seau", f"{equilibre_moyen:.0f} MWh/jour")
                st.metric("Prix moyen march√©", f"{df_energie['prix_marche_‚Ç¨_mwh'].mean():.1f} ‚Ç¨/MWh")
            
            # Graphiques production/consommation
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.line(df_energie.tail(30), x='date', y=['consommation_mwh', 'production_totale'],
                            title="Production vs Consommation (derniers 30 jours)")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Mix √©nerg√©tique
                mix_energetique = {
                    'Solaire': df_energie['production_solaire'].mean(),
                    '√âolien': df_energie['production_eolienne'].mean(),
                    'Gaz': df_energie['production_gaz'].mean()
                }
                
                fig = px.pie(values=list(mix_energetique.values()), 
                           names=list(mix_energetique.keys()),
                           title="Mix √©nerg√©tique moyen")
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.subheader("üîß Maintenance Pr√©dictive des Infrastructures")
            
            # Simulation de donn√©es de maintenance
            n_equipements = 50
            data_maintenance = {
                'equipement_id': range(1, n_equipements + 1),
                'type_equipement': np.random.choice(['Transformateur', 'Ligne HT', 'Sous-station', 'G√©n√©rateur'], n_equipements),
                'age_ans': np.random.randint(1, 30, n_equipements),
                'temperature_moyenne': np.random.normal(65, 15, n_equipements),
                'vibrations': np.random.normal(2, 0.5, n_equipements),
                'niveau_isolation': np.random.normal(85, 10, n_equipements),
                'heures_fonctionnement': np.random.randint(1000, 50000, n_equipements)
            }
            
            df_maintenance = pd.DataFrame(data_maintenance)
            
            # Calcul du risque de panne (mod√®le simplifi√©)
            df_maintenance['risque_panne'] = (
                df_maintenance['age_ans'] * 0.03 +
                (df_maintenance['temperature_moyenne'] - 60) * 0.01 +
                (df_maintenance['vibrations'] - 2) * 0.1 +
                (100 - df_maintenance['niveau_isolation']) * 0.02
            )
            df_maintenance['risque_panne'] = df_maintenance['risque_panne'].clip(0, 1)
            df_maintenance['priorite_maintenance'] = pd.cut(df_maintenance['risque_panne'], 
                                                          bins=[0, 0.3, 0.7, 1], 
                                                          labels=['Basse', 'Moyenne', 'Haute'])
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**√âquipements √† risque √©lev√©**")
                equipements_risque = df_maintenance[df_maintenance['priorite_maintenance'] == 'Haute'].sort_values('risque_panne', ascending=False)
                st.dataframe(equipements_risque[['equipement_id', 'type_equipement', 'age_ans', 'risque_panne']].head(10))
            
            with col2:
                risque_par_type = df_maintenance.groupby('type_equipement')['risque_panne'].mean().sort_values(ascending=False)
                fig = px.bar(risque_par_type, 
                           title="Risque de panne moyen par type d'√©quipement")
                st.plotly_chart(fig, use_container_width=True)
            
            # Calculateur de risque d'√©quipement
            st.subheader("üîÆ Calculateur de Risque de Panne")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                age = st.slider("√Çge √©quipement (ans)", 1, 30, 10)
                temperature = st.slider("Temp√©rature moyenne (¬∞C)", 40, 100, 65)
            
            with col2:
                vibrations = st.slider("Niveau vibrations", 1.0, 5.0, 2.0)
                isolation = st.slider("Niveau isolation (%)", 50, 100, 85)
            
            with col3:
                heures_fonc = st.slider("Heures fonctionnement", 1000, 50000, 25000)
                type_eq = st.selectbox("Type √©quipement", ['Transformateur', 'Ligne HT', 'Sous-station', 'G√©n√©rateur'])
            
            # Calcul du risque (mod√®le FACTICE pour d√©monstration)
            risque_calc = (
                age * 0.03 +
                (temperature - 60) * 0.01 +
                (vibrations - 2) * 0.1 +
                (100 - isolation) * 0.02
            )
            risque_calc = min(0.95, max(0.05, risque_calc))
            
            st.metric("Risque de panne estim√©", f"{risque_calc:.1%}")
            
            if risque_calc > 0.7:
                st.error("üî¥ RISQUE √âLEV√â - Maintenance urgente requise")
                st.write("**Actions recommand√©es:** Inspection imm√©diate, R√©duction charge, Plan remplacement")
            elif risque_calc > 0.4:
                st.warning("üü° RISQUE MOD√âR√â - Surveillance renforc√©e")
                st.write("**Actions recommand√©es:** Monitoring temp√©rature, Planning maintenance pr√©ventive")
            else:
                st.success("üü¢ RISQUE FAIBLE - Maintenance normale")
                st.write("**Actions recommand√©es:** Maintenance programm√©e, Surveillance standard")
        
        with tab3:
            st.subheader("üí∞ Optimisation des Achats d'√ânergie")
            
            # Analyse des prix de march√©
            st.write("**Analyse des Prix de March√© et Optimisation des Achats**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Volatilit√© des prix
                df_energie['prix_rolling_avg'] = df_energie['prix_marche_‚Ç¨_mwh'].rolling(7).mean()
                df_energie['prix_rolling_std'] = df_energie['prix_marche_‚Ç¨_mwh'].rolling(7).std()
                
                fig = px.line(df_energie.tail(90), x='date', 
                            y=['prix_marche_‚Ç¨_mwh', 'prix_rolling_avg'],
                            title="√âvolution des prix de march√© (90 derniers jours)")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Corr√©lation temp√©rature/prix
                fig = px.scatter(df_energie, x='temperature', y='prix_marche_‚Ç¨_mwh',
                               trendline='ols',
                               title="Impact de la temp√©rature sur les prix",
                               trendline_color_override='red')
                st.plotly_chart(fig, use_container_width=True)
            
            # Recommandations d'optimisation
            st.subheader("üí° Strat√©gies d'Optimisation")
            
            prix_moyen = df_energie['prix_marche_‚Ç¨_mwh'].mean()
            prix_min = df_energie['prix_marche_‚Ç¨_mwh'].min()
            prix_max = df_energie['prix_marche_‚Ç¨_mwh'].max()
            volatilite = df_energie['prix_marche_‚Ç¨_mwh'].std()
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Prix moyen", f"{prix_moyen:.1f} ‚Ç¨/MWh")
                st.write("**Achats de base** - Contrats long terme")
            
            with col2:
                st.metric("Prix minimum", f"{prix_min:.1f} ‚Ç¨/MWh")
                st.write("**Achats spot** - P√©riodes creuses")
            
            with col3:
                st.metric("Prix maximum", f"{prix_max:.1f} ‚Ç¨/MWh")
                st.write("**Production interne** - P√©riodes de pic")
            
            with col4:
                st.metric("Volatilit√©", f"{volatilite:.1f} ‚Ç¨")
                st.write("**Couverture risque** - Instruments financiers")
            
            # Strat√©gie d'optimisation
            st.write("""
            **Strat√©gie recommand√©e:**
            - **60%** en contrats long terme (stabilit√©)
            - **25%** en achats spot (optimisation co√ªts)  
            - **10%** en production de r√©serve (s√©curit√©)
            - **5%** en instruments de couverture (gestion risque)
            
            **√âconomies potentielles:** 15-25% sur les co√ªts d'approvisionnement
            
            **Facteurs cl√©s de succ√®s:**
            - Surveillance continue des march√©s
            - Pr√©visions m√©t√©orologiques pr√©cises
            - Analyse en temps r√©el de la consommation
            - Maintenance pr√©ventive des infrastructures
            """)


# =============================================================================
# SECTION 4: SCIENCE DES DONN√âES APPLIQU√âE - CORRIG√âE ET COMPL√âT√âE
# =============================================================================
# =============================================================================
# SECTION 4: SCIENCE DES DONN√âES APPLIQU√âE - CORRIG√âE ET COMPL√âT√âE
# =============================================================================

def section_science_donnees():
    st.header("üî¨ Science des Donn√©es Appliqu√©e")
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìà Types d'Analytics", "ü§ñ Machine Learning", "üìä Visualisation", "üßπ Data Quality", "üéØ Cas R√©els"
    ])
    
    with tab1:
        st.subheader("Les 4 Types d'Analytics")
        
        # Charger les donn√©es pour cette section
        df_clients, df_transactions = generer_donnees_retail()
        
        analytics_type = st.radio(
            "Choisir un type d'analytics:",
            ["Descriptive (Que s'est-il pass√©?)", 
             "Diagnostic (Pourquoi c'est arriv√©?)", 
             "Predictive (Que va-t-il se passer?)", 
             "Prescriptive (Que devrions-nous faire?)"]
        )
        
        if "Descriptive" in analytics_type:
            st.info("""
            **üìä Analytics Descriptive : Comprendre le pass√©**
            
            **Techniques:**
            - Reporting et tableaux de bord
            - M√©triques KPI traditionnelles
            - Analyse de tendances
            
            **Outils:** Tableau, Power BI, Google Analytics
            **Cas d'usage:** Rapports de performance, suivi d'activit√©
            """)
            
            # Exemple de dataset descriptif
            data = pd.DataFrame({
                'Produit': ['A', 'B', 'C', 'D', 'E'],
                'Ventes': [120, 150, 130, 170, 190],
                'Croissance': [5, 12, -2, 8, 15]
            })
            
            fig = px.bar(data, x='Produit', y='Ventes', color='Croissance',
                        title="Performance des Produits - Analytics Descriptive")
            st.plotly_chart(fig, use_container_width=True)
        
        elif "Diagnostic" in analytics_type:
            st.warning("""
            **üîç Analytics Diagnostic : Comprendre les causes**
            
            **Techniques:**
            - Analyse de corr√©lation
            - Analyse de racine cause
            - Segmentation avanc√©e
            
            **Cas d'usage:** Identifier pourquoi les ventes ont baiss√©
            """)
            
            # Analyse diagnostique
            df_analysis = df_clients.merge(
                df_transactions.groupby('client_id').agg({
                    'montant': 'sum'
                }).reset_index(), 
                left_on='client_id', 
                right_on='client_id'
            )
            
            fig = px.scatter(df_analysis, x='revenu_annuel', y='montant', 
                           color='segment', trendline='ols',
                           title="Corr√©lation Revenu vs D√©penses")
            st.plotly_chart(fig, use_container_width=True)
        
        elif "Predictive" in analytics_type:
            st.success("""
            **üîÆ Analytics Predictive : Anticiper le futur**
            
            **Techniques:**
            - Machine Learning
            - Mod√®les statistiques
            - S√©ries temporelles
            
            **Cas d'usage:** Pr√©vision des ventes, pr√©diction de churn
            """)
            
            # Simulation pr√©dictive
            df_clients['score_churn'] = np.random.beta(2, 5, len(df_clients))
            
            fig = px.histogram(df_clients, x='score_churn', 
                             title="Distribution du Score de Churn Pr√©dit")
            st.plotly_chart(fig, use_container_width=True)
        
        elif "Prescriptive" in analytics_type:
            st.error("""
            **üí° Analytics Prescriptive : Recommander des actions**
            
            **Techniques:**
            - Optimisation
            - Simulation
            - Syst√®mes experts
            
            **Cas d'usage:** Optimisation des campagnes, recommandations strat√©giques
            """)
            
            # Exemple prescriptif
            recommendations = pd.DataFrame({
                'Segment': ['VIP', 'Fid√®le', 'R√©gulier', 'Occasionnel'],
                'Action': ['Offre exclusive', 'Programme fid√©lit√©', 'Email personnalis√©', 'Campagne reactivation'],
                'Budget': [5000, 3000, 1500, 2000],
                'ROI_attendu': [2.5, 1.8, 1.2, 0.8]
            })
            
            fig = px.bar(recommendations, x='Segment', y='ROI_attendu',
                        title="ROI Attendu par Type d'Action Marketing")
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.subheader("ü§ñ Machine Learning Op√©rationnel")
        
        ml_type = st.selectbox(
            "Type de ML:",
            ["Supervis√©", "Non-supervis√©", "Renforcement", "Deep Learning"]
        )
        
        if ml_type == "Supervis√©":
            st.markdown("""
            **Apprentissage Supervis√©**
            
            **Algorithme:** Pr√©diction bas√©e sur des donn√©es labellis√©es
            
            **Applications:**
            - Classification: Spam detection, diagnostic m√©dical
            - R√©gression: Pr√©vision ventes, pricing
            """)
            
            # Simulation de mod√®le de pr√©diction
            st.subheader("Simulateur de Pr√©diction de Churn")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                age = st.slider("√Çge du client", 18, 80, 35)
                anciennete = st.slider("Anciennet√© (mois)", 1, 60, 12)
            
            with col2:
                revenu = st.slider("Revenu annuel (k‚Ç¨)", 20, 150, 50)
                satisfaction = st.slider("Satisfaction (1-10)", 1, 10, 7)
            
            with col3:
                frequence = st.slider("Fr√©quence achats/mois", 1, 20, 5)
                panier_moyen = st.slider("Panier moyen (‚Ç¨)", 20, 200, 85)
            
            # Mod√®le simplifi√© de pr√©diction
            proba_churn = max(0.05, min(0.95, 
                0.3 + 
                (age * 0.001) + 
                (anciennete * -0.01) + 
                ((150 - revenu) * 0.002) +
                ((10 - satisfaction) * 0.03) +
                ((10 - frequence) * 0.02) +
                ((200 - panier_moyen) * 0.001)
            ))
            
            st.metric("Probabilit√© de Churn", f"{proba_churn:.1%}")
            
            if proba_churn > 0.7:
                st.error("Risque √âLEV√â - Actions imm√©diates requises")
            elif proba_churn > 0.4:
                st.warning("Risque MOYEN - Surveillance recommand√©e")
            else:
                st.success("Risque FAIBLE - Client stable")
        
        elif ml_type == "Non-supervis√©":
            st.success("**üîç Apprentissage Non Supervis√© : D√©couvrir des patterns cach√©s**")
            
            st.markdown("""
            **Use Case :** Segmentation de client√®le sans labels pr√©-d√©finis
            **Algorithme :** K-Means Clustering
            **Applications :** Marketing personnalis√©, D√©tection d'anomalies
            """)
            
            # G√©n√©rer des donn√©es pour le clustering
            df_clients, _ = generer_donnees_retail()
            
            # Clustering des clients
            df_clusters = df_clients[['age', 'revenu_annuel', 'frequence_achats', 'panier_moyen']].copy()
            scaler = StandardScaler()
            df_scaled = scaler.fit_transform(df_clusters)
            
            kmeans = KMeans(n_clusters=4, random_state=42)
            df_clusters['cluster'] = kmeans.fit_predict(df_scaled)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.scatter(df_clusters, x='revenu_annuel', y='panier_moyen', 
                               color='cluster', size='frequence_achats',
                               title="Segmentation client par revenu et comportement")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Profils des clusters
                cluster_profiles = df_clusters.groupby('cluster').mean()
                fig = px.bar(cluster_profiles.T, barmode='group',
                           title="Profils moyens des clusters")
                st.plotly_chart(fig, use_container_width=True)
            
            st.subheader("üìã Description des Segments D√©couverts")
            segments_desc = {
                0: "üéØ Jeunes actifs - Revenu moyen, forte fr√©quence d'achat",
                1: "üíé Clients premium - Revenu √©lev√©, panier important",
                2: "üìä Clients √©conomiques - Revenu faible, recherche de bonnes affaires",
                3: "‚≠ê Clients fid√®les - Anciennet√© √©lev√©e, fr√©quence stable"
            }
            
            for cluster, desc in segments_desc.items():
                st.write(f"**Cluster {cluster}:** {desc}")
        
        elif ml_type == "Renforcement":
            st.info("**üéÆ Apprentissage par Renforcement : Apprentissage par l'exp√©rience**")
            
            st.markdown("""
            **Use Case :** Optimisation des recommandations produits en temps r√©el
            **Algorithme :** Q-Learning, Deep Q-Network
            **Applications :** Syst√®mes de recommandation, Trading algorithmique
            """)
            
            # Simulation d'apprentissage par renforcement
            st.subheader("Simulation : Optimisation des Recommandations")
            
            # √âtats (segments clients) et Actions (types de recommandations)
            etats = ['Jeune', 'Actif', 'Senior', 'VIP']
            actions = ['Promo', 'Nouveaut√©', 'Cross-selling', 'Service premium']
            
            # Matrice Q simul√©e
            q_table = pd.DataFrame(
                np.random.uniform(0, 1, (len(etats), len(actions))),
                index=etats,
                columns=actions
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Matrice Q (Apprentissage)**")
                st.dataframe(q_table.style.background_gradient(cmap='Blues'))
            
            with col2:
                # Simulation d'√©pisode
                etat_courant = st.selectbox("Segment client actuel:", etats)
                action_choisie = q_table.loc[etat_courant].idxmax()
                
                st.metric("Meilleure action recommand√©e", action_choisie)
                st.metric("Confiance du mod√®le", f"{q_table.loc[etat_courant, action_choisie]:.3f}")
                
                # Feedback d'apprentissage
                satisfaction = st.slider("Satisfaction client (r√©compense):", 0.0, 1.0, 0.8)
                
                if st.button("Mettre √† jour l'apprentissage"):
                    # Mise √† jour Q-learning simplifi√©e
                    q_table.loc[etat_courant, action_choisie] = satisfaction
                    st.success("Mod√®le mis √† jour avec le feedback!")
        
        elif ml_type == "Deep Learning":
            st.error("**üß† Deep Learning : R√©seaux de neurones profonds**")
            
            st.markdown("""
            **Applications:**
            - Vision par ordinateur
            - Traitement du langage naturel
            - Reconnaissance vocale
            
            **Cas d'usage:**
            - Classification d'images
            - Traduction automatique
            - Chatbots intelligents
            """)
            
            # Simulation de r√©seau de neurones
            st.subheader("Architecture de R√©seau de Neurones")
            
            layers = st.slider("Nombre de couches cach√©es", 1, 5, 3)
            neurons = st.slider("Neurones par couche", 10, 100, 50)
            
            st.write(f"**Architecture:** 10 ‚Üí {' ‚Üí '.join([str(neurons)] * layers)} ‚Üí 1")
            st.metric("Complexit√© du mod√®le", f"{(10 * neurons + neurons * neurons * (layers-1) + neurons * 1):,} param√®tres")
    
    with tab3:
        st.subheader("üìä Visualisation Avanc√©e des Donn√©es")
        
        viz_type = st.selectbox(
            "Type de visualisation:",
            ["Analyse Temporelle", "Cartographie", "Network Analysis", "Dashboard Interactif"]
        )
        
        if viz_type == "Analyse Temporelle":
            st.success("**üìà Analyse des Tendances Temporelles**")
            
            # G√©n√©rer des donn√©es temporelles
            _, df_transactions = generer_donnees_retail()
            
            # Agr√©gation par mois
            df_transactions['mois'] = df_transactions['date'].dt.to_period('M')
            trends = df_transactions.groupby('mois').agg({
                'montant': ['sum', 'count'],
                'client_id': 'nunique'
            }).reset_index()
            
            trends.columns = ['mois', 'ca_total', 'nb_transactions', 'clients_uniques']
            trends['mois'] = trends['mois'].astype(str)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.line(trends, x='mois', y='ca_total', 
                            title="√âvolution du Chiffre d'Affaires Mensuel")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                fig = px.area(trends, x='mois', y=['nb_transactions', 'clients_uniques'],
                            title="Activit√© commerciale mensuelle")
                st.plotly_chart(fig, use_container_width=True)
        
        elif viz_type == "Cartographie":
            st.info("**üó∫Ô∏è Visualisation G√©ographique et Cartographie**")
            
            # Donn√©es g√©ographiques simul√©es pour le S√©n√©gal
            regions_senegal = {
                'Region': ['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor', 'Kaolack', 'Kolda', 'Matam', 'Fatick', 'Louga'],
                'Latitude': [14.7167, 14.8, 14.75, 16.0167, 12.5833, 14.15, 12.8833, 15.1167, 14.3333, 15.6167],
                'Longitude': [-17.4672, -16.9667, -16.2333, -16.5, -16.2667, -16.0833, -14.95, -13.65, -16.4167, -16.2167],
                'Ventes': [500000, 250000, 180000, 150000, 120000, 100000, 80000, 70000, 60000, 50000],
                'Clients': [15000, 8000, 6000, 5000, 4000, 3500, 3000, 2500, 2000, 1500]
            }
            
            df_geo = pd.DataFrame(regions_senegal)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Carte choropl√®the
                fig = px.scatter_geo(df_geo, 
                                   lat='Latitude', 
                                   lon='Longitude',
                                   size='Ventes',
                                   color='Clients',
                                   hover_name='Region',
                                   scope='africa',
                                   title="Performance Commerciale par R√©gion S√©n√©gal",
                                   size_max=30)
                fig.update_geos(visible=False, resolution=50,
                              showcountries=True, countrycolor="Black",
                              showsubunits=True, subunitcolor="Blue")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Carte de chaleur g√©ographique
                fig = px.density_mapbox(df_geo, 
                                      lat='Latitude', 
                                      lon='Longitude', 
                                      z='Ventes',
                                      radius=20,
                                      center=dict(lat=14.5, lon=-14.5),
                                      zoom=5,
                                      mapbox_style="stamen-terrain",
                                      title="Densit√© des Ventes par R√©gion")
                st.plotly_chart(fig, use_container_width=True)
            
            # Cartographie des flux
            st.subheader("üìä Cartographie des Flux Logistiques")
            
            # Simulation de flux entre r√©gions
            flux_data = {
                'Source': ['Dakar', 'Dakar', 'Thi√®s', 'Saint-Louis', 'Dakar'],
                'Target': ['Thi√®s', 'Saint-Louis', 'Diourbel', 'Ziguinchor', 'Kaolack'],
                'Volume': [500, 300, 200, 150, 250],
                'Cout': [10000, 15000, 8000, 20000, 12000]
            }
            
            df_flux = pd.DataFrame(flux_data)
            
            fig = px.line_geo(df_flux,
                            lat=[14.7167, 14.8, 14.75, 16.0167, 12.5833, 14.15],
                            lon=[-17.4672, -16.9667, -16.2333, -16.5, -16.2667, -16.0833],
                            projection="natural earth")
            st.plotly_chart(fig, use_container_width=True)
        
        elif viz_type == "Network Analysis":
            st.warning("**üï∏Ô∏è Analyse de R√©seaux et Relations**")
            
            st.markdown("""
            **Applications:**
            - Analyse des relations clients
            - D√©tection de communaut√©s
            - Optimisation des r√©seaux logistiques
            """)
            
            # Simulation d'un r√©seau social de clients
            np.random.seed(42)
            n_nodes = 30
            
            # Cr√©ation d'un graph de r√©seau
            G = nx.erdos_renyi_graph(n_nodes, 0.1)
            pos = nx.spring_layout(G)
            
            # Attributs des n≈ìuds
            for i in range(n_nodes):
                G.nodes[i]['segment'] = np.random.choice(['VIP', 'Fid√®le', 'R√©gulier', 'Occasionnel'])
                G.nodes[i]['valeur'] = np.random.randint(1000, 50000)
            
            # Visualisation avec plotly
            edge_x = []
            edge_y = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None])
                edge_y.extend([y0, y1, None])
            
            edge_trace = go.Scatter(
                x=edge_x, y=edge_y,
                line=dict(width=0.5, color='#888'),
                hoverinfo='none',
                mode='lines')
            
            node_x = []
            node_y = []
            node_text = []
            node_color = []
            for node in G.nodes():
                x, y = pos[node]
                node_x.append(x)
                node_y.append(y)
                node_text.append(f"Client {node}<br>Segment: {G.nodes[node]['segment']}<br>Valeur: {G.nodes[node]['valeur']}‚Ç¨")
                node_color.append(G.nodes[node]['valeur'])
            
            node_trace = go.Scatter(
                x=node_x, y=node_y,
                mode='markers',
                hoverinfo='text',
                marker=dict(
                    showscale=True,
                    colorscale='YlGnBu',
                    size=10,
                    colorbar=dict(
                        thickness=15,
                        title='Valeur Client',
                        xanchor='left',
                        titleside='right'
                    ),
                    line_width=2))
            
            node_trace.text = node_text
            node_trace.marker.color = node_color
            
            fig = go.Figure(data=[edge_trace, node_trace],
                          layout=go.Layout(
                              title='R√©seau des Relations Clients',
                              showlegend=False,
                              hovermode='closest',
                              margin=dict(b=20,l=5,r=5,t=40),
                              annotations=[ dict(
                                  text="Analyse des communaut√©s clients",
                                  showarrow=False,
                                  xref="paper", yref="paper",
                                  x=0.005, y=-0.002 ) ],
                              xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                              yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                          )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # M√©triques du r√©seau
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Nombre de n≈ìuds", G.number_of_nodes())
            with col2:
                st.metric("Nombre de liens", G.number_of_edges())
            with col3:
                st.metric("Densit√© du r√©seau", f"{nx.density(G):.3f}")
        
        elif viz_type == "Dashboard Interactif":
            st.success("**üì± Tableau de Bord Interactif Temps R√©el**")
            
            # Simulation de donn√©es temps r√©el
            st.subheader("üìä Tableau de Bord Commercial Temps R√©el")
            
            # G√©n√©ration de donn√©es dynamiques
            np.random.seed(42)
            heures = list(range(24))
            data_temps_reel = {
                'Heure': heures,
                'Visites': np.random.poisson(100, 24) + np.sin(np.array(heures) * np.pi / 12) * 50,
                'Conversions': np.random.poisson(20, 24) + np.sin(np.array(heures) * np.pi / 12) * 10,
                'Chiffre_Affaires': np.random.normal(5000, 1000, 24) + np.sin(np.array(heures) * np.pi / 12) * 2000
            }
            
            df_temps_reel = pd.DataFrame(data_temps_reel)
            
            # M√©triques en temps r√©el
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                visites_actuelles = df_temps_reel['Visites'].iloc[-1]
                st.metric("Visites actuelles", f"{visites_actuelles}", "+5%")
            
            with col2:
                conversions_actuelles = df_temps_reel['Conversions'].iloc[-1]
                st.metric("Conversions", f"{conversions_actuelles}", "+3%")
            
            with col3:
                ca_actuel = df_temps_reel['Chiffre_Affaires'].iloc[-1]
                st.metric("Chiffre d'Affaires", f"{ca_actuel:.0f} ‚Ç¨", "+8%")
            
            with col4:
                taux_conversion = (conversions_actuelles / visites_actuelles) * 100
                st.metric("Taux Conversion", f"{taux_conversion:.1f}%", "+0.5%")
            
            # Graphiques interactifs
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.line(df_temps_reel, x='Heure', y=['Visites', 'Conversions'],
                            title="Activit√© Commerciale sur 24h")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                fig = px.area(df_temps_reel, x='Heure', y='Chiffre_Affaires',
                            title="√âvolution du Chiffre d'Affaires")
                st.plotly_chart(fig, use_container_width=True)
            
            # Filtres interactifs
            st.subheader("üîß Filtres Avanc√©s")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                segment_filter = st.multiselect(
                    "Segment client:",
                    ['VIP', 'Fid√®le', 'R√©gulier', 'Occasionnel'],
                    default=['VIP', 'Fid√®le']
                )
            
            with col2:
                region_filter = st.selectbox(
                    "R√©gion:",
                    ['Toutes'] + ['Dakar', 'Thi√®s', 'Diourbel', 'Saint-Louis', 'Ziguinchor']
                )
            
            with col3:
                date_range = st.date_input(
                    "P√©riode:",
                    [datetime(2024, 1, 1), datetime(2024, 12, 31)]
                )
            
            # Alertes et notifications
            st.subheader("üö® Alertes et Recommandations")
            
            alert_col1, alert_col2, alert_col3 = st.columns(3)
            
            with alert_col1:
                if taux_conversion < 15:
                    st.error("‚ö†Ô∏è Taux de conversion bas")
                else:
                    st.success("‚úÖ Taux de conversion optimal")
            
            with alert_col2:
                if visites_actuelles < 80:
                    st.warning("üìâ Traffic faible - V√©rifier le r√©f√©rencement")
                else:
                    st.success("üìà Traffic normal")
            
            with alert_col3:
                if ca_actuel < 4000:
                    st.error("üí∞ CA en baisse - Actions marketing requises")
                else:
                    st.success("üí∞ CA dans les objectifs")
    
    with tab4:
        st.subheader("üßπ Data Quality - Diagnostic Complet")
        
        df_qualite = generer_donnees_qualite()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**√âchantillon des donn√©es avec probl√®mes**")
            st.dataframe(df_qualite.head(10))
        
        with col2:
            # M√©triques de qualit√©
            completude = (1 - df_qualite.isnull().mean()) * 100
            validite = pd.Series({
                'email': (df_qualite['email'].str.contains('@', na=False).mean()) * 100,
                'telephone': (df_qualite['telephone'].str.len() == 9).mean() * 100,
                'age': ((df_qualite['age'] >= 18) & (df_qualite['age'] <= 100)).mean() * 100,
                'salaire': (df_qualite['salaire'] > 0).mean() * 100
            })
            
            st.write("**M√©triques de Qualit√©**")
            for col in completude.index:
                st.metric(f"Compl√©tude {col}", f"{completude[col]:.1f}%")
            
            st.write("**Validit√© des donn√©es**")
            for col, score in validite.items():
                st.metric(f"Validit√© {col}", f"{score:.1f}%")
        
        # Recommandations d'am√©lioration
        st.subheader("üîß Plan d'Am√©lioration de la Qualit√©")
        
        problems = []
        if completude['email'] < 95:
            problems.append("üìß Impl√©menter la validation des emails en temps r√©el")
        if validite['age'] < 95:
            problems.append("üéÇ Ajouter des contr√¥les de plage pour l'√¢ge")
        if completude['ville'] < 95:
            problems.append("üèôÔ∏è Utiliser l'autocompl√©tion pour les villes")
        
        for problem in problems:
            st.write(f"- {problem}")
    
    with tab5:
        st.subheader("üéØ Cas R√©els d'Impl√©mentation")
        
        cas = st.selectbox(
            "Choisir un cas r√©el:",
            ["Netflix - Syst√®me de recommandation", 
             "Uber - Optimisation des trajets",
             "Amazon - D√©tection de fraude",
             "Airbnb - Pricing dynamique",
             "Tesla - Conduite autonome"]
        )
        
        if "Netflix" in cas:
            st.success("**üé¨ Netflix : Syst√®me de Recommandation Avanc√©**")
            
            st.markdown("""
            **Challenge :** Personnaliser l'exp√©rience pour 200M+ d'utilisateurs
            **Solution :** Algorithmes de collaborative filtering + Deep Learning
            **R√©sultats :**
            - 80% du contenu visionn√© vient des recommandations
            - R√©duction du churn de 25%
            - Augmentation du temps de visionnage de 30%
            """)
            
            # Simulation des algorithmes Netflix
            st.subheader("Simulation d'Algorithme de Recommandation")
            
            films = ['Action Hero', 'Romantic Story', 'Sci-Fi Adventure', 'Comedy Night', 'Documentary World']
            user_profiles = {
                'Jeune': [0.9, 0.3, 0.8, 0.7, 0.2],
                'Famille': [0.6, 0.7, 0.5, 0.9, 0.6],
                'Cin√©phile': [0.7, 0.8, 0.9, 0.6, 0.9]
            }
            
            profile = st.selectbox("Profil utilisateur:", list(user_profiles.keys()))
            scores = user_profiles[profile]
            
            fig = px.bar(x=films, y=scores, title=f"Recommandations pour profil {profile}",
                       labels={'x': 'Films', 'y': 'Score de recommandation'})
            st.plotly_chart(fig, use_container_width=True)
        
        elif "Uber" in cas:
            st.info("**üöó Uber : Optimisation des Trajets en Temps R√©el**")
            
            st.markdown("""
            **Challenge :** Minimiser les temps d'attente et maximiser l'efficacit√© des trajets
            **Solution :** Algorithmes de matching + Optimisation de routes en temps r√©el
            **R√©sultats :**
            - R√©duction temps d'attente moyen de 40%
            - Augmentation du nombre de courses par chauffeur de 25%
            - Optimisation du prix dynamique
            """)
            
            # Simulation d'optimisation de trajets Uber
            st.subheader("Simulation d'Optimisation de Trajets")
            
            # Donn√©es de simulation
            np.random.seed(42)
            n_requests = 20
            n_drivers = 15
            
            requests_data = {
                'request_id': range(n_requests),
                'pickup_lat': np.random.uniform(14.6, 14.8, n_requests),
                'pickup_lon': np.random.uniform(-17.5, -17.3, n_requests),
                'dropoff_lat': np.random.uniform(14.6, 14.8, n_requests),
                'dropoff_lon': np.random.uniform(-17.5, -17.3, n_requests),
                'priority': np.random.choice(['Haute', 'Moyenne', 'Basse'], n_requests, p=[0.2, 0.5, 0.3])
            }
            
            drivers_data = {
                'driver_id': range(n_drivers),
                'current_lat': np.random.uniform(14.6, 14.8, n_drivers),
                'current_lon': np.random.uniform(-17.5, -17.3, n_drivers),
                'status': np.random.choice(['Libre', 'En course'], n_drivers, p=[0.6, 0.4])
            }
            
            df_requests = pd.DataFrame(requests_data)
            df_drivers = pd.DataFrame(drivers_data)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Carte des demandes
                fig = px.scatter_mapbox(df_requests, 
                                      lat="pickup_lat", 
                                      lon="pickup_lon",
                                      color="priority",
                                      size_max=15,
                                      zoom=12,
                                      mapbox_style="open-street-map",
                                      title="Demandes de course √† Dakar")
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Optimisation du matching
                st.write("**Optimisation du Matching Conducteurs-Clients**")
                
                # Algorithme simplifi√© de matching
                free_drivers = len(df_drivers[df_drivers['status'] == 'Libre'])
                waiting_requests = len(df_requests)
                
                st.metric("Conducteurs disponibles", free_drivers)
                st.metric("Demandes en attente", waiting_requests)
                st.metric("Temps d'attente moyen", "3.2 min")
                st.metric("Taux de matching", "92%")
                
                if st.button("Lancer l'optimisation"):
                    st.success("‚úÖ Matching optimis√© !")
                    st.write("""
                    **R√©sultats de l'optimisation:**
                    - 18 demandes assign√©es
                    - Temps d'attente moyen r√©duit √† 2.1 min
                    - Distance totale √©conomis√©e: 15.7 km
                    """)
        
        elif "Amazon" in cas:
            st.error("**üì¶ Amazon : D√©tection de Fraude Avanc√©e**")
            
            st.markdown("""
            **Challenge :** Prot√©ger 300M+ de comptes contre la fraude
            **Solution :** Machine Learning temps r√©el + Analyse comportementale
            **R√©sultats :**
            - R√©duction des fraudes de 75%
            - Faux positifs r√©duits √† moins de 0.1%
            - √âconomies de 1.2 milliard $ par an
            """)
            
            # Simulation de d√©tection de fraude Amazon
            st.subheader("Simulation de D√©tection de Fraude")
            
            # G√©n√©ration de donn√©es de transactions
            np.random.seed(42)
            n_transactions = 1000
            
            fraud_data = {
                'transaction_id': range(n_transactions),
                'montant': np.random.exponential(50, n_transactions),
                'distance_domicile': np.random.exponential(10, n_transactions),
                'heure': np.random.randint(0, 24, n_transactions),
                'pays': np.random.choice(['S√©n√©gal', 'France', 'USA', 'Autre'], n_transactions, p=[0.6, 0.2, 0.1, 0.1]),
                'nouveau_merchant': np.random.choice([0, 1], n_transactions, p=[0.7, 0.3])
            }
            
            df_fraud = pd.DataFrame(fraud_data)
            
            # Calcul du score de fraude (mod√®le simplifi√©)
            df_fraud['score_fraude'] = (
                (df_fraud['montant'] > 200) * 0.3 +
                (df_fraud['distance_domicile'] > 50) * 0.2 +
                ((df_fraud['heure'] < 6) | (df_fraud['heure'] > 22)) * 0.2 +
                (df_fraud['pays'] == 'Autre') * 0.2 +
                (df_fraud['nouveau_merchant'] == 1) * 0.1 +
                np.random.normal(0, 0.05, n_transactions)
            )
            
            df_fraud['alerte_fraude'] = df_fraud['score_fraude'] > 0.7
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Distribution des scores de fraude
                fig = px.histogram(df_fraud, x='score_fraude', 
                                 color='alerte_fraude',
                                 title="Distribution des Scores de Fraude",
                                 nbins=30)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # M√©triques de performance
                vraies_alertes = df_fraud['alerte_fraude'].sum()
                taux_fraude = (vraies_alertes / len(df_fraud)) * 100
                
                st.metric("Transactions analys√©es", f"{n_transactions:,}")
                st.metric("Alertes de fraude", vraies_alertes)
                st.metric("Taux de fraude d√©tect√©", f"{taux_fraude:.2f}%")
                st.metric("Pr√©cision du mod√®le", "99.2%")
            
            # Test de transaction suspecte
            st.subheader("üîç Testez une Transaction")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                montant_test = st.slider("Montant (‚Ç¨)", 1, 500, 100)
                heure_test = st.slider("Heure de la transaction", 0, 23, 12)
            
            with col2:
                distance_test = st.slider("Distance du domicile (km)", 0, 100, 10)
                pays_test = st.selectbox("Pays", ['S√©n√©gal', 'France', 'USA', 'Autre'])
            
            with col3:
                nouveau_merchant_test = st.checkbox("Nouveau marchand")
            
            # Calcul du score
            score_test = (
                (montant_test > 200) * 0.3 +
                (distance_test > 50) * 0.2 +
                ((heure_test < 6) | (heure_test > 22)) * 0.2 +
                (pays_test == 'Autre') * 0.2 +
                nouveau_merchant_test * 0.1
            )
            
            st.metric("Score de risque de fraude", f"{score_test:.2f}")
            
            if score_test > 0.7:
                st.error("üö® TRANSACTION SUSPECTE - V√©rification requise")
                st.write("**Actions recommand√©es:** V√©rification 2FA, Contact client, Retard traitement")
            elif score_test > 0.4:
                st.warning("‚ö†Ô∏è RISQUE MOD√âR√â - Surveillance recommand√©e")
            else:
                st.success("‚úÖ TRANSACTION NORMALE - Traitement imm√©diat")
        
        elif "Airbnb" in cas:
            st.warning("**üè† Airbnb : Pricing Dynamique Intelligent**")
            
            st.markdown("""
            **Challenge :** Optimiser les prix pour 7M+ de logements worldwide
            **Solution :** Machine Learning + Analyse du march√© en temps r√©el
            **R√©sultats :**
            - Augmentation des revenus des h√¥tes de 20-40%
            - Am√©lioration du taux d'occupation de 15%
            - Satisfaction client am√©lior√©e
            """)
            
            # Simulation de pricing dynamique Airbnb
            st.subheader("Simulateur de Pricing Dynamique")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Facteurs influen√ßant le prix
                type_logement = st.selectbox("Type de logement", 
                                           ['Appartement', 'Maison', 'Studio', 'Villa'])
                nb_chambres = st.slider("Nombre de chambres", 1, 5, 2)
                equipements = st.multiselect("√âquipements",
                                           ['WiFi', 'Piscine', 'Parking', 'Climatisation', 'Cuisine √©quip√©e'])
                
            with col2:
                localisation = st.selectbox("Localisation", 
                                          ['Dakar Plateau', 'Almadies', 'Ngor', 'Ouest Foire', 'Sacre Coeur'])
                saison = st.selectbox("Saison", 
                                    ['Basse', 'Moyenne', 'Haute', 'Tr√®s haute'])
                evenements = st.checkbox("√âv√©nements sp√©ciaux dans la r√©gion")
            
            # Calcul du prix optimis√©
            prix_base = 50  # Prix de base en ‚Ç¨
            
            # Facteurs multiplicatifs
            facteurs = {
                'type_logement': {'Appartement': 1.0, 'Maison': 1.3, 'Studio': 0.8, 'Villa': 2.0},
                'nb_chambres': {1: 0.8, 2: 1.0, 3: 1.3, 4: 1.6, 5: 2.0},
                'localisation': {'Dakar Plateau': 1.2, 'Almadies': 1.5, 'Ngor': 1.4, 'Ouest Foire': 1.0, 'Sacre Coeur': 1.3},
                'saison': {'Basse': 0.7, 'Moyenne': 1.0, 'Haute': 1.5, 'Tr√®s haute': 2.0},
                'equipements': len(equipements) * 0.1,
                'evenements': 1.2 if evenements else 1.0
            }
            
            prix_optimise = prix_base * (
                facteurs['type_logement'][type_logement] *
                facteurs['nb_chambres'][nb_chambres] *
                facteurs['localisation'][localisation] *
                facteurs['saison'][saison] *
                (1 + facteurs['equipements']) *
                facteurs['evenements']
            )
            
            # Comparaison avec le march√©
            prix_marche = prix_optimise * np.random.uniform(0.8, 1.2)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Prix recommand√©", f"{prix_optimise:.0f} ‚Ç¨/nuit")
            
            with col2:
                st.metric("Prix moyen march√©", f"{prix_marche:.0f} ‚Ç¨/nuit")
            
            with col3:
                difference = ((prix_optimise - prix_marche) / prix_marche) * 100
                st.metric("√âcart march√©", f"{difference:+.1f}%")
            
            # Recommandations
            st.subheader("üí° Recommandations de Pricing")
            
            if difference > 10:
                st.error("Prix TROP √âLEV√â - Risque de faible occupation")
                st.write("**Actions:** R√©viser √† la baisse, Offres promotionnelles")
            elif difference < -10:
                st.warning("Prix TROP BAS - Manque √† gagner")
                st.write("**Actions:** Augmenter progressivement, Mettre en valeur les atouts")
            else:
                st.success("Prix OPTIMAL - Bon √©quilibre")
                st.write("**Actions:** Maintenir la strat√©gie, Surveiller la concurrence")
            
            # Analyse de sensibilit√©
            st.subheader("üìà Analyse de Sensibilit√© au Prix")
            
            prix_test = np.linspace(prix_optimise * 0.5, prix_optimise * 1.5, 10)
            occupation_test = 100 - (prix_test - prix_optimise) / prix_optimise * 50
            occupation_test = np.clip(occupation_test, 10, 95)
            revenu_test = prix_test * occupation_test / 100
            
            df_sensibilite = pd.DataFrame({
                'Prix': prix_test,
                'Taux_Occupation': occupation_test,
                'Revenu_Relatif': revenu_test
            })
            
            fig = px.line(df_sensibilite, x='Prix', y=['Taux_Occupation', 'Revenu_Relatif'],
                         title="Impact du Prix sur l'Occupation et le Revenu",
                         labels={'value': 'Pourcentage', 'variable': 'M√©trique'})
            st.plotly_chart(fig, use_container_width=True)
        
        elif "Tesla" in cas:
            st.success("**üöó Tesla : Conduite Autonome par Deep Learning**")
            
            st.markdown("""
            **Challenge :** D√©velopper une conduite autonome de niveau 4-5
            **Solution :** R√©seaux de neurones convolutionnels + Reinforcement Learning
            **R√©sultats :**
            - 8 milliards de miles parcourus en mode autonome
            - R√©duction des accidents de 40%
            - Am√©lioration continue via learning f√©d√©r√©
            """)
            
            # Simulation de conduite autonome Tesla
            st.subheader("Simulation de Perception Autonome")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Simulation des capteurs
                st.write("**Donn√©es des Capteurs**")
                
                # Radar
                st.metric("Distance v√©hicule avant", "25.3 m")
                st.metric("Vitesse relative", "-2.1 km/h")
                
                # Cam√©ras
                st.metric("D√©tection pi√©tons", "3")
                st.metric("D√©tection v√©hicules", "5")
                st.metric("Reconnaissance feux", "Vert")
                
                # Lidar
                st.metric("Pr√©cision carte 3D", "98.7%")
                st.metric("Points cloud/s", "2.4M")
            
            with col2:
                # D√©cisions de conduite
                st.write("**D√©cisions de Conduite**")
                
                vitesses = {
                    'Limite l√©gale': '50 km/h',
                    'Vitesse conseill√©e': '45 km/h', 
                    'Vitesse actuelle': '43 km/h'
                }
                
                for decision, valeur in vitesses.items():
                    st.metric(decision, valeur)
                
                st.write("**Actions en cours:**")
                st.write("‚úÖ Maintenir la voie")
                st.write("‚úÖ Distance de s√©curit√©")
                st.write("üîÑ Adaptation vitesse trafic")
                st.write("‚úÖ Surveillance pi√©tons")
            
            # Simulation d'apprentissage
            st.subheader("üß† Apprentissage du Mod√®le Autonome")
            
            # Donn√©es d'apprentissage simul√©es
            learning_data = {
                '√âpoque': range(1, 101),
                'Pr√©cision': np.minimum(0.95, 0.3 + 0.65 * (1 - np.exp(-np.arange(100)/20)) + np.random.normal(0, 0.02, 100)),
                'Perte': np.maximum(0.05, 2.0 * np.exp(-np.arange(100)/15) + np.random.normal(0, 0.1, 100))
            }
            
            df_learning = pd.DataFrame(learning_data)
            
            fig = px.line(df_learning, x='√âpoque', y=['Pr√©cision', 'Perte'],
                         title="Courbe d'Apprentissage du Mod√®le Autonome",
                         labels={'value': 'Valeur', 'variable': 'M√©trique'})
            st.plotly_chart(fig, use_container_width=True)
            
            # M√©triques de s√©curit√©
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Kilom√©trage autonome", "8.2B miles")
            with col2:
                st.metric("Interventions humaines", "1/5M miles")
            with col3:
                st.metric("Fiabilit√© perception", "99.98%")

# =============================================================================
# SECTION 5: GOUVERNANCE ET BEST PRACTICES
# =============================================================================

def section_gouvernance():
    st.header("üèõÔ∏è Gouvernance des Donn√©es")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Cadre de Gouvernance")
        
        st.markdown("""
        **1. Qualit√© des Donn√©es**
        - Exactitude et compl√©tude
        - Consistance et int√©grit√©
        - Actualit√© et disponibilit√©
        
        **2. S√©curit√© et Confidentialit√©**
        - Chiffrement des donn√©es
        - Contr√¥le d'acc√®s RBAC
        - Audit et conformit√© RGPD
        
        **3. M√©tadonn√©es et Lineage**
        - Catalogue de donn√©es
        - Tra√ßabilit√© des transformations
        - Documentation automatique
        """)
    
    with col2:
        st.subheader("√âvaluation de la Maturit√© Data")
        
        aspects = {
            "Strat√©gie Data": st.slider("Strat√©gie Data", 1, 5, 3),
            "Gouvernance": st.slider("Gouvernance", 1, 5, 2),
            "Qualit√©": st.slider("Qualit√© des donn√©es", 1, 5, 3),
            "S√©curit√©": st.slider("S√©curit√©", 1, 5, 4),
            "Culture Data": st.slider("Culture Data", 1, 5, 2)
        }
        
        if st.button("Calculer le Score de Maturit√©"):
            score = sum(aspects.values()) / (len(aspects) * 5) * 100
            st.metric("Score de Maturit√© Data", f"{score:.1f}%")
            
            if score >= 80:
                st.success("üéØ Niveau Expert - Focus optimisation")
            elif score >= 60:
                st.info("üìà Niveau Avanc√© - D√©veloppement capacit√©s")
            elif score >= 40:
                st.warning("üìö Niveau Interm√©diaire - Renforcement")
            else:
                st.error("üå± Niveau D√©butant - Construction fondations")

# =============================================================================
# SECTION 6: IMPL√âMENTATION ET ROI
# =============================================================================

def section_implementation():
    st.header("üöÄ Impl√©mentation et Calcul de ROI")
    
    st.subheader("Calculateur de ROI Big Data")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Investissements**")
        cout_infrastructure = st.number_input("Infrastructure (k‚Ç¨)", min_value=10, value=100)
        cout_formation = st.number_input("Formation (k‚Ç¨)", min_value=5, value=30)
        cout_maintenance = st.number_input("Maintenance annuelle (k‚Ç¨)", min_value=10, value=50)
        
        st.write("**Gains Attendus**")
        gain_efficacite = st.slider("Gain d'efficacit√© (%)", 1, 50, 15)
        augmentation_ventes = st.slider("Augmentation ventes (%)", 1, 30, 10)
        reduction_couts = st.slider("R√©duction co√ªts (%)", 1, 40, 12)
    
    with col2:
        # Calculs ROI
        investissement_total = cout_infrastructure + cout_formation + cout_maintenance
        gains_annuels = (gain_efficacite + augmentation_ventes + reduction_couts) / 3 * investissement_total / 10
        
        roi = (gains_annuels - cout_maintenance) / investissement_total * 100
        payback_period = investissement_total / gains_annuels if gains_annuels > 0 else float('inf')
        
        st.metric("ROI Annuel Estim√©", f"{roi:.1f}%")
        st.metric("P√©riode de Retour", f"{payback_period:.1f} ans")
        st.metric("Gains Annuels Nets", f"{gains_annuels - cout_maintenance:.0f} k‚Ç¨")
        
        # Recommandations
        if roi > 50:
            st.success("üéØ ROI Excellent - Projet tr√®s attractif")
        elif roi > 20:
            st.info("üìà ROI Bon - Projet recommand√©")
        else:
            st.warning("‚ö†Ô∏è ROI Faible - R√©√©valuer le projet")

# =============================================================================
# APPLICATION PRINCIPALE
# =============================================================================

def main():
    # Sidebar Navigation
    st.sidebar.title("üìö Big Data MBA Complet")
    
    menu_options = [
        "üåç Introduction au Big Data",
        "üõ†Ô∏è Technologies et Architectures", 
        "üè¢ Cas Pratiques par Secteur",
        "üî¨ Science des Donn√©es Appliqu√©e",
        "üèõÔ∏è Gouvernance et Best Practices",
        "üöÄ Impl√©mentation et ROI"
    ]
    
    choix = st.sidebar.radio("Navigation:", menu_options)
    
    # Chargement des donn√©es
    df = generer_donnees_big_data()
    
    # Router vers la section s√©lectionn√©e
    if choix == menu_options[0]:
        section_introduction()
    elif choix == menu_options[1]:
        section_technologies()
    elif choix == menu_options[2]:
        section_cas_pratiques()
    elif choix == menu_options[3]:
        section_science_donnees()
    elif choix == menu_options[4]:
        section_gouvernance()
    elif choix == menu_options[5]:
        section_implementation()
    
    # Footer avec m√©triques
    st.sidebar.markdown("---")
    st.sidebar.subheader("üìä M√©triques Globales")
    
    metrics = calculer_metrics_5v(df)
    for metric, value in metrics.items():
        if metric == 'V√©racit√©':
            st.sidebar.metric(metric, f"{value:.1f}%")
        elif metric == 'Valeur':
            st.sidebar.metric(metric, f"{value:.2f}")
        else:
            st.sidebar.metric(metric, f"{value:,.0f}")





if __name__ == "__main__":
    main()
